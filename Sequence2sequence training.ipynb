{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sequence to Sequence Learning with Neural Networks* in PyTorch\n",
    "\n",
    "Encoder-decoder sequence to sequence RNN implementation based on 2014 publication:\n",
    "\n",
    "**Sequence to Sequence Learning with Neural Networks** - Ilya Sutskever, Oriol Vinyals, Quoc V. Le\n",
    "\n",
    "https://arxiv.org/abs/1409.3215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In the project files(PyTorch dataset implementation fra_eng_dataset.py) I use small toy dataset(170K sentences) in fra-eng folder. But for this experiment I will try to use **WMT'14 English-German** dataset (4.5M sentences)\n",
    "\n",
    "The dataset can be founde here:\n",
    "https://nlp.stanford.edu/projects/nmt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import os\n",
    "\n",
    "\n",
    "RNN_LAYERS = 4\n",
    "RNN_HIDDEN_SIZE = 1024\n",
    "IN_EMBEDDING_SIZE = 128\n",
    "OUT_EMBEDDING_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "MAXMAX_SENTENCE_LEN = 20\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_dictionary(text_corpus_path, top_n = 50000):\n",
    "\n",
    "    print(f\"Creating top dictionary from: {text_corpus_path}..\")\n",
    "    \n",
    "    last_token_idx = 0\n",
    "    token_dict = dict()\n",
    "    token_counts_list = []\n",
    "    token_idx_to_token = []\n",
    "    \n",
    "    with open(text_corpus_path, \"r\", encoding='utf-8') as f:\n",
    "        \n",
    "        for idx, line in enumerate(f.readlines()):\n",
    "            \n",
    "            if (idx+1)% 500000 == 0:\n",
    "                print(f\"Processed {idx+1} lines\")\n",
    "\n",
    "            line = line.replace('##AT##', '')\n",
    "            token_list = word_tokenize(line)\n",
    "            for token in token_list:\n",
    "                token = token.lower()\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = last_token_idx\n",
    "                    token_idx_to_token.append(token)\n",
    "                    token_counts_list.append((0,last_token_idx))\n",
    "                    last_token_idx += 1\n",
    "\n",
    "                token_idx = token_dict[token]\n",
    "                count, _ = token_counts_list[token_idx]\n",
    "                token_counts_list[token_idx] = (count+1,token_idx)\n",
    "                \n",
    "    token_counts_list = sorted(token_counts_list, reverse=True)\n",
    "    \n",
    "    top_token_list = []\n",
    "    \n",
    "    for idx, (count, token_idx) in enumerate(token_counts_list):\n",
    "        top_token_list.append(token_idx_to_token[token_idx])\n",
    "        \n",
    "        if idx > top_n:\n",
    "            break\n",
    "    \n",
    "    return top_token_list\n",
    "        \n",
    "\n",
    "\n",
    "class WMT14_en_de_Dataset(Dataset):\n",
    "    def __init__(self, data_source_path = 'wmt14_en_de'):\n",
    "        super().__init__()\n",
    "        \n",
    "        processed_data_path = \"processed_data.pkl\"\n",
    "        top_en_tokens_path = \"top_en_tokens.pkl\"\n",
    "        top_de_tokens_path = \"top_de_tokens.pkl\"\n",
    "        \n",
    "        self.sentence_list = []\n",
    "        \n",
    "        self.en_token_dict = dict()\n",
    "        self.en_token_dict['<PAD>'] = 0\n",
    "        self.en_token_dict['<EOS>'] = 1\n",
    "        self.en_token_dict['<UNK>'] = 2\n",
    "        self.en_last_token_idx = 2\n",
    "        self.en_token_idx_to_text = ['<PAD>', '<EOS>', '<UNK>']\n",
    "        \n",
    "        self.de_token_dict = dict()\n",
    "        self.de_token_dict['<PAD>'] = 0\n",
    "        self.de_token_dict['<EOS>'] = 1\n",
    "        self.de_token_dict['<UNK>'] = 2\n",
    "        self.de_last_token_idx = 2\n",
    "        self.de_token_idx_to_text = ['<PAD>', '<EOS>', '<UNK>']\n",
    "        \n",
    "        \n",
    "        if os.path.exists(processed_data_path):\n",
    "            with open(processed_data_path, 'rb') as f:\n",
    "                pickle_data = pickle.load(f)\n",
    "                self.sentence_list = pickle_data['sentence_list']\n",
    "                self.en_last_token_idx = pickle_data['en_last_token_idx']\n",
    "                self.de_last_token_idx = pickle_data['de_last_token_idx']\n",
    "                self.en_token_idx_to_text = pickle_data['en_token_idx_to_text']\n",
    "                self.de_token_idx_to_text = pickle_data['de_token_idx_to_text']\n",
    "        else:\n",
    "        \n",
    "            en_sentences_path = os.path.join(data_source_path, \"train.en\")\n",
    "            de_sentences_path = os.path.join(data_source_path, \"train.de\")\n",
    "            \n",
    "            if os.path.exists(top_en_tokens_path):\n",
    "                with open(top_en_tokens_path, \"rb\") as f:\n",
    "                    top_en_tokens = pickle.load(f)\n",
    "            else:\n",
    "                top_en_tokens = get_top_dictionary(en_sentences_path)\n",
    "                with open(top_en_tokens_path, \"wb\") as f:\n",
    "                    pickle.dump(top_en_tokens, f)\n",
    "            \n",
    "            \n",
    "            for token in top_en_tokens:\n",
    "                self.en_last_token_idx += 1\n",
    "                self.en_token_dict[token] = self.en_last_token_idx\n",
    "                self.en_token_idx_to_text.append(token)\n",
    "                \n",
    " \n",
    "            if os.path.exists(top_de_tokens_path):\n",
    "                with open(top_de_tokens_path, \"rb\") as f:\n",
    "                    top_de_tokens = pickle.load(f)\n",
    "            else:\n",
    "                top_de_tokens = get_top_dictionary(de_sentences_path)\n",
    "                with open(top_de_tokens_path, \"wb\") as f:\n",
    "                    pickle.dump(top_de_tokens, f)\n",
    "            \n",
    "            for token in top_de_tokens:\n",
    "                self.de_last_token_idx += 1\n",
    "                self.de_token_dict[token] = self.de_last_token_idx\n",
    "                self.de_token_idx_to_text.append(token)         \n",
    "                    \n",
    "            \n",
    "            with open(de_sentences_path, \"r\", encoding='utf-8') as de_f:\n",
    "                with open(en_sentences_path, \"r\", encoding='utf-8') as en_f:\n",
    "                    \n",
    "                    print(\"Creating sentences from {de_sentences_path} and {en_sentences_path} coropuses\")\n",
    "                    \n",
    "                    for idx, (de_sentence, en_sentence) in enumerate(zip(de_f.readlines(), en_f.readlines())):\n",
    "                        \n",
    "                        if (idx+1)%500000 == 0:\n",
    "                            print(f\"Processed {idx+1} lines\")\n",
    "                            \n",
    "                        de_sentence = de_sentence.replace('##AT##', '')\n",
    "                        en_sentence = en_sentence.replace('##AT##', '')\n",
    "                        \n",
    "                        en_token_sentence = []\n",
    "                        de_token_sentence = []\n",
    "\n",
    "                        en_token_list = word_tokenize(en_sentence)\n",
    "                        for token in en_token_list:\n",
    "                            token = token.lower()\n",
    "                            if token in self.en_token_dict:\n",
    "                                token_idx = self.en_token_dict[token]\n",
    "                            else:\n",
    "                                token_idx = self.en_token_dict['<UNK>']\n",
    "                                \n",
    "                            en_token_sentence.append(token_idx)\n",
    "\n",
    "                        en_token_sentence.append(self.en_token_dict['<EOS>'])\n",
    "\n",
    "                        de_token_list = word_tokenize(de_sentence)\n",
    "                        for token in de_token_list:\n",
    "                            token = token.lower()\n",
    "                            if token in self.de_token_dict:\n",
    "                                token_idx = self.de_token_dict[token]\n",
    "                            else:\n",
    "                                token_idx = self.de_token_dict['<UNK>']\n",
    "                                    \n",
    "\n",
    "                            de_token_sentence.append(token_idx)\n",
    "\n",
    "                        de_token_sentence.append(self.de_token_dict['<EOS>'])\n",
    "\n",
    "                        self.sentence_list.append(\n",
    "                            dict(\n",
    "                                en = en_token_sentence,\n",
    "                                de = de_token_sentence\n",
    "                            ))\n",
    "                        \n",
    "            with open(processed_data_path, \"wb\") as f:\n",
    "                pickle_processed_data = dict(\n",
    "                    sentence_list = self.sentence_list,\n",
    "                    en_last_token_idx = self.en_last_token_idx,\n",
    "                    de_last_token_idx = self.de_last_token_idx,\n",
    "                    en_token_idx_to_text = self.en_token_idx_to_text,\n",
    "                    de_token_idx_to_text = self.de_token_idx_to_text\n",
    "                )\n",
    "                pickle.dump(pickle_processed_data, f)\n",
    "            \n",
    "    def get_en_dict_size(self):\n",
    "        return self.en_last_token_idx + 1\n",
    "        \n",
    "    def get_de_dict_size(self):\n",
    "        return self.de_last_token_idx + 1\n",
    "    \n",
    "    def get_de_eos_code(self):\n",
    "        return self.de_token_dict['<EOS>']\n",
    "    \n",
    "    def get_en_eos_code(self):\n",
    "        return self.en_token_dict['<EOS>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        ret = dict()\n",
    "        for key in self.sentence_list[item]:\n",
    "            ret[key] = torch.tensor(self.sentence_list[item][key])\n",
    "        return ret\n",
    "\n",
    "\n",
    "def en_de_dataset_collate(data):\n",
    "\n",
    "    en_sentences = []\n",
    "    en_sentence_lens = []\n",
    "    de_sentences = []\n",
    "    de_sentence_lens = []\n",
    "    \n",
    "    en_sentences_sorted = []\n",
    "    en_sentence_lens_sorted = []\n",
    "    de_sentences_sorted = []\n",
    "    de_sentence_lens_sorted = []\n",
    "    \n",
    "    for s in data:\n",
    "        \n",
    "        sent = s['en'][0:MAXMAX_SENTENCE_LEN]\n",
    "        en_sentences.append(sent.unsqueeze(dim=1))\n",
    "        en_sentence_lens.append(len(sent))\n",
    "        \n",
    "        sent = s['de'][0:MAXMAX_SENTENCE_LEN]\n",
    "        de_sentences.append(sent.unsqueeze(dim=1))\n",
    "        de_sentence_lens.append(len(sent))\n",
    "\n",
    "    #Rearrange everything by de sentence lens\n",
    "    sort_idxes = np.argsort(np.array(de_sentence_lens))[::-1]\n",
    "    for idx in sort_idxes:\n",
    "        en_sentences_sorted.append(en_sentences[idx])\n",
    "        en_sentence_lens_sorted.append(en_sentence_lens[idx])\n",
    "        de_sentences_sorted.append(de_sentences[idx])\n",
    "        de_sentence_lens_sorted.append(de_sentence_lens[idx])\n",
    "    \n",
    "    return dict(\n",
    "        en_sentences = en_sentences_sorted,\n",
    "        en_lens = en_sentence_lens_sorted,\n",
    "        de_sentences = de_sentences_sorted,\n",
    "        de_lens = de_sentence_lens_sorted\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_encoder_model(nn.Module):\n",
    "    def __init__(self, in_dict_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dict_size = in_dict_size\n",
    "\n",
    "        self.embedding = nn.Linear(\n",
    "            in_dict_size, \n",
    "            IN_EMBEDDING_SIZE)\n",
    "        \n",
    "        \n",
    "        self.hidden = None \n",
    "        self.cell = None\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = IN_EMBEDDING_SIZE,\n",
    "            hidden_size = RNN_HIDDEN_SIZE,\n",
    "            num_layers = RNN_LAYERS\n",
    "        )\n",
    "        \n",
    "    def init_hidden_and_cell(self):\n",
    "        self.hidden = torch.randn(RNN_LAYERS, BATCH_SIZE, RNN_HIDDEN_SIZE).to(device)\n",
    "        self.cell = torch.rand(RNN_LAYERS, BATCH_SIZE, RNN_HIDDEN_SIZE).to(device)\n",
    "    \n",
    "    def get_hidden_and_cell(self):\n",
    "        return self.hidden, self.cell\n",
    "    \n",
    "    def forward(self, x):\n",
    "        padded_sent_one_hot, sent_lens = x\n",
    "        padded_sent_emb = self.embedding.forward(padded_sent_one_hot)\n",
    "        packed = pack_padded_sequence(padded_sent_emb, sent_lens)\n",
    "        packed, (self.hidden, self.cell) = self.rnn.forward(packed, (self.hidden,self.cell))\n",
    "        padded, sent_lens = pad_packed_sequence(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_decoder_model(nn.Module):\n",
    "    def __init__(self, out_dict_size):\n",
    "        super().__init__()\n",
    "      \n",
    "        self.in_embedding = nn.Linear(\n",
    "            in_features=out_dict_size,\n",
    "            out_features=IN_EMBEDDING_SIZE\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = IN_EMBEDDING_SIZE,\n",
    "            hidden_size = RNN_HIDDEN_SIZE,\n",
    "            num_layers = RNN_LAYERS\n",
    "        )\n",
    "\n",
    "        self.rnn_to_embedding = nn.Linear(\n",
    "            in_features = RNN_HIDDEN_SIZE,\n",
    "            out_features = OUT_EMBEDDING_SIZE\n",
    "        )\n",
    "\n",
    "        self.embedding_to_logit = nn.Linear(\n",
    "            in_features = OUT_EMBEDDING_SIZE, \n",
    "            out_features = out_dict_size\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def init_hidden_and_cell(self, hidden, cell):\n",
    "        self.hidden = hidden\n",
    "        self.cell = cell\n",
    "    \n",
    "    \n",
    "    def forward(self, out_eos_code, out_dict_size, max_sentence_len):\n",
    "        batch_size = self.hidden.shape[1]\n",
    "        prev_outp = (torch.ones(1, batch_size, 1) * out_eos_code).long()\n",
    "        prev_outp = prev_outp.to(device)\n",
    "        \n",
    "        all_outp_prob = []\n",
    "        \n",
    "        for timestep in range(max_sentence_len):\n",
    "            \n",
    "            prev_outp_one_hot = torch.zeros(prev_outp.shape[0], prev_outp.shape[1], out_dict_size).to(device)\n",
    "            prev_outp_one_hot = prev_outp_one_hot.scatter_(2,prev_outp.data,1)\n",
    "            \n",
    "            prev_outp_in_emb = self.in_embedding(prev_outp_one_hot)\n",
    "         \n",
    "            cur_outp_hid, (self.hidden, self.cell) = self.rnn.forward(prev_outp_in_emb, (self.hidden, self.cell))\n",
    "            cur_outp_emb = self.rnn_to_embedding.forward(cur_outp_hid)\n",
    "            cur_outp_logits = self.embedding_to_logit(cur_outp_emb)\n",
    "            cur_outp_prob = self.softmax(cur_outp_logits)\n",
    "            all_outp_prob.append(cur_outp_prob)\n",
    "            \n",
    "            prev_outp = torch.argmax(cur_outp_prob.data.to(device), dim=2, keepdim=True)\n",
    "        \n",
    "        all_outp_prob_tensor = torch.cat(all_outp_prob, dim=0)\n",
    "    \n",
    "        return all_outp_prob_tensor\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WMT14_en_de_Dataset()\n",
    "sentences_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=en_de_dataset_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(in_sentence_list, out_sentence_list, pred_tensor, num_batches):\n",
    "    \n",
    "    sentence_prediction_samples_path = 'sentence_predictions.text'\n",
    "    \n",
    "    print(f\"Printing sample predictions after {num_batches} batches in {sentence_prediction_samples_path}\")\n",
    "    \n",
    "    with open(sentence_prediction_samples_path, \"a\") as f:\n",
    "\n",
    "        for i in range(3):\n",
    "            f.write('='*50 + '\\n')\n",
    "            \n",
    "        f.write(f\"Sample predictions after {num_batches} batches \\n\\n\")\n",
    "              \n",
    "        in_token_to_text = dataset.de_token_idx_to_text\n",
    "        out_token_to_text = dataset.en_token_idx_to_text\n",
    "\n",
    "        for s in range(min(len(in_sentence_list),50)):\n",
    "\n",
    "            in_sent_text = []\n",
    "            for in_token in in_sentence_list[s].squeeze():\n",
    "                in_sent_text.append(in_token_to_text[in_token])\n",
    "\n",
    "            f.write(f\"\\nGerman sentence is: {' '.join(in_sent_text)} \\n\")\n",
    "\n",
    "            out_sent_text = []\n",
    "\n",
    "            for out_token in out_sentence_list[s].squeeze():\n",
    "                  out_sent_text.append(out_token_to_text[out_token])\n",
    "            f.write(f\"English sentence is: {' '.join(out_sent_text)}\\n\")\n",
    "\n",
    "            pred_sent_text = []\n",
    "            for ts in range(pred_tensor.shape[0]):\n",
    "                pred_token = torch.argmax(pred_tensor[ts, s,:]).data\n",
    "                pred_sent_text.append(out_token_to_text[pred_token])\n",
    "\n",
    "                if pred_token == dataset.get_en_eos_code():\n",
    "                    break\n",
    "            f.write(f\"Translated English sentence is: {' '.join(pred_sent_text)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_encoder = RNN_encoder_model(dataset.get_de_dict_size()).to(device)\n",
    "rnn_decoder = RNN_decoder_model(dataset.get_en_dict_size()).to(device)\n",
    "\n",
    "trained_encoder_path = None\n",
    "trained_decoder_path = None\n",
    "\n",
    "trained_encoder_path = 'models/encoder_wmt14_de_en.pt'\n",
    "trained_decoder_path = 'models/decoder_wmt14_de_en.pt'\n",
    "\n",
    "if os.path.exists(trained_encoder_path):\n",
    "    rnn_encoder.load_state_dict(torch.load(trained_encoder_path))\n",
    "if os.path.exists(trained_decoder_path):\n",
    "    rnn_decoder.load_state_dict(torch.load(trained_decoder_path))\n",
    "\n",
    "\n",
    "params = list(rnn_encoder.parameters()) + list(rnn_decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 =====================\n",
      "200 Average loss in the last 200 batches is 15070.2412109375\n",
      "400 Average loss in the last 200 batches is 14894.48828125\n",
      "600 Average loss in the last 200 batches is 14763.2783203125\n",
      "800 Average loss in the last 200 batches is 14697.6083984375\n",
      "1000 Average loss in the last 200 batches is 14628.3720703125\n",
      "1200 Average loss in the last 200 batches is 14548.44921875\n",
      "1400 Average loss in the last 200 batches is 14447.9462890625\n",
      "1600 Average loss in the last 200 batches is 14417.166015625\n",
      "1800 Average loss in the last 200 batches is 14295.33203125\n",
      "2000 Average loss in the last 200 batches is 14311.87109375\n",
      "Printing sample predictions after 2000 batches in sentence_predictions.text\n",
      "2200 Average loss in the last 200 batches is 14228.9013671875\n",
      "2400 Average loss in the last 200 batches is 14185.505859375\n",
      "2600 Average loss in the last 200 batches is 14131.49609375\n",
      "2800 Average loss in the last 200 batches is 14131.35546875\n",
      "3000 Average loss in the last 200 batches is 14102.1298828125\n",
      "3200 Average loss in the last 200 batches is 14002.0009765625\n",
      "3400 Average loss in the last 200 batches is 13970.92578125\n",
      "3600 Average loss in the last 200 batches is 13859.416015625\n",
      "3800 Average loss in the last 200 batches is 13848.23046875\n",
      "4000 Average loss in the last 200 batches is 13818.4150390625\n",
      "Printing sample predictions after 4000 batches in sentence_predictions.text\n",
      "4200 Average loss in the last 200 batches is 13837.005859375\n",
      "4400 Average loss in the last 200 batches is 13749.712890625\n",
      "4600 Average loss in the last 200 batches is 13668.41796875\n",
      "4800 Average loss in the last 200 batches is 13670.275390625\n",
      "5000 Average loss in the last 200 batches is 13591.1171875\n",
      "5200 Average loss in the last 200 batches is 13566.8896484375\n",
      "5400 Average loss in the last 200 batches is 13574.1015625\n",
      "5600 Average loss in the last 200 batches is 13535.5009765625\n",
      "5800 Average loss in the last 200 batches is 13497.0947265625\n",
      "6000 Average loss in the last 200 batches is 13489.90625\n",
      "Printing sample predictions after 6000 batches in sentence_predictions.text\n",
      "6200 Average loss in the last 200 batches is 13419.607421875\n",
      "6400 Average loss in the last 200 batches is 13431.134765625\n",
      "6600 Average loss in the last 200 batches is 13382.369140625\n",
      "6800 Average loss in the last 200 batches is 13366.490234375\n",
      "7000 Average loss in the last 200 batches is 13344.4345703125\n",
      "7200 Average loss in the last 200 batches is 13284.025390625\n",
      "7400 Average loss in the last 200 batches is 13317.83984375\n",
      "7600 Average loss in the last 200 batches is 13259.5029296875\n",
      "7800 Average loss in the last 200 batches is 13226.2138671875\n",
      "8000 Average loss in the last 200 batches is 13204.005859375\n",
      "Printing sample predictions after 8000 batches in sentence_predictions.text\n",
      "8200 Average loss in the last 200 batches is 13174.76171875\n",
      "8400 Average loss in the last 200 batches is 13136.51171875\n",
      "8600 Average loss in the last 200 batches is 13140.337890625\n",
      "8800 Average loss in the last 200 batches is 13126.94140625\n",
      "9000 Average loss in the last 200 batches is 13055.1328125\n",
      "9200 Average loss in the last 200 batches is 13040.318359375\n",
      "9400 Average loss in the last 200 batches is 13045.068359375\n",
      "9600 Average loss in the last 200 batches is 13053.724609375\n",
      "9800 Average loss in the last 200 batches is 13026.0859375\n",
      "10000 Average loss in the last 200 batches is 12989.9521484375\n",
      "Printing sample predictions after 10000 batches in sentence_predictions.text\n",
      "10200 Average loss in the last 200 batches is 12969.65625\n",
      "10400 Average loss in the last 200 batches is 12924.568359375\n",
      "10600 Average loss in the last 200 batches is 12936.4990234375\n",
      "10800 Average loss in the last 200 batches is 12865.7001953125\n",
      "11000 Average loss in the last 200 batches is 12847.3251953125\n",
      "11200 Average loss in the last 200 batches is 12840.6171875\n",
      "11400 Average loss in the last 200 batches is 12838.8427734375\n",
      "11600 Average loss in the last 200 batches is 12806.7412109375\n",
      "11800 Average loss in the last 200 batches is 12777.7802734375\n",
      "12000 Average loss in the last 200 batches is 12768.1904296875\n",
      "Printing sample predictions after 12000 batches in sentence_predictions.text\n",
      "12200 Average loss in the last 200 batches is 12784.490234375\n",
      "12400 Average loss in the last 200 batches is 12725.4873046875\n",
      "12600 Average loss in the last 200 batches is 12705.9013671875\n",
      "12800 Average loss in the last 200 batches is 12729.021484375\n",
      "13000 Average loss in the last 200 batches is 12680.712890625\n",
      "13200 Average loss in the last 200 batches is 12659.47265625\n",
      "13400 Average loss in the last 200 batches is 12617.7890625\n",
      "13600 Average loss in the last 200 batches is 12630.712890625\n",
      "13800 Average loss in the last 200 batches is 12586.7197265625\n",
      "14000 Average loss in the last 200 batches is 12615.494140625\n",
      "Printing sample predictions after 14000 batches in sentence_predictions.text\n",
      "14200 Average loss in the last 200 batches is 12617.052734375\n",
      "14400 Average loss in the last 200 batches is 12598.51171875\n",
      "14600 Average loss in the last 200 batches is 12581.708984375\n",
      "14800 Average loss in the last 200 batches is 12551.0009765625\n",
      "15000 Average loss in the last 200 batches is 12530.7373046875\n",
      "15200 Average loss in the last 200 batches is 12495.93359375\n",
      "15400 Average loss in the last 200 batches is 12479.4765625\n",
      "15600 Average loss in the last 200 batches is 12456.2509765625\n",
      "15800 Average loss in the last 200 batches is 12403.982421875\n",
      "16000 Average loss in the last 200 batches is 12466.369140625\n",
      "Printing sample predictions after 16000 batches in sentence_predictions.text\n",
      "16200 Average loss in the last 200 batches is 12415.1904296875\n",
      "16400 Average loss in the last 200 batches is 12392.5400390625\n",
      "16600 Average loss in the last 200 batches is 12392.0361328125\n",
      "16800 Average loss in the last 200 batches is 12321.048828125\n",
      "17000 Average loss in the last 200 batches is 12328.6337890625\n",
      "17200 Average loss in the last 200 batches is 12317.33984375\n",
      "17400 Average loss in the last 200 batches is 12370.6650390625\n",
      "17600 Average loss in the last 200 batches is 12326.8935546875\n",
      "17800 Average loss in the last 200 batches is 12278.7841796875\n",
      "18000 Average loss in the last 200 batches is 12300.2421875\n",
      "Printing sample predictions after 18000 batches in sentence_predictions.text\n",
      "18200 Average loss in the last 200 batches is 12244.3837890625\n",
      "18400 Average loss in the last 200 batches is 12320.8427734375\n",
      "18600 Average loss in the last 200 batches is 12271.40234375\n",
      "18800 Average loss in the last 200 batches is 12235.1865234375\n",
      "19000 Average loss in the last 200 batches is 12268.7158203125\n",
      "19200 Average loss in the last 200 batches is 12228.982421875\n",
      "19400 Average loss in the last 200 batches is 12167.607421875\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "num_batches = 0\n",
    "num_loss_prints = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"Starting epoch {epoch} =====================\")\n",
    "    \n",
    "    best_loss = 1e10\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for idx, sentences in enumerate(sentences_loader):\n",
    "\n",
    "        rnn_encoder.init_hidden_and_cell()\n",
    "       \n",
    "   \n",
    "        in_sentences = sentences['de_sentences']\n",
    "        in_lens = sentences['de_lens']\n",
    "        out_sentences = sentences['en_sentences']\n",
    "        out_lens = sentences['en_lens']\n",
    "        \n",
    "\n",
    "        padded_in = pad_sequence(in_sentences, padding_value=0).to(device)\n",
    "        padded_out = pad_sequence(out_sentences, padding_value=0).to(device)\n",
    "\n",
    "        padded_in_one_hot = torch.zeros(padded_in.shape[0], padded_in.shape[1], dataset.get_de_dict_size()).to(device)\n",
    "        padded_in_one_hot = padded_in_one_hot.scatter_(2,padded_in.data,1)\n",
    "       \n",
    "        rnn_encoder.forward((padded_in_one_hot, in_lens))\n",
    "        hidden, cell = rnn_encoder.get_hidden_and_cell()\n",
    "       \n",
    "        rnn_decoder.init_hidden_and_cell(hidden,cell)\n",
    "       \n",
    "        max_sentence_len = padded_out.shape[0]\n",
    "            \n",
    "        y_pred = rnn_decoder.forward(dataset.get_en_eos_code(), dataset.get_en_dict_size(), max_sentence_len)\n",
    "       \n",
    "        padded_out = padded_out[0:max_sentence_len]\n",
    "        padded_out_one_hot = torch.zeros(padded_out.shape[0], padded_out.shape[1], dataset.get_en_dict_size()).to(device)\n",
    "        padded_out_one_hot = padded_out_one_hot.scatter_(2,padded_out.data,1)\n",
    "       \n",
    "        #Make all padded one-hot vectors to all zeros, which will make\n",
    "        #padded components loss 0 and so they wont affect the loss\n",
    "        padded_out_one_hot[:,:,0] = torch.zeros(max_sentence_len, padded_out_one_hot.shape[1])\n",
    "        loss = torch.sum(-torch.log(y_pred + 1e-9) * padded_out_one_hot)\n",
    "       \n",
    "        loss_sum += loss.to('cpu').detach().data\n",
    "       \n",
    "        #print(loss.to('cpu').detach().data)\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Every 100 batches print the average loss and store the model weights\n",
    "        steps += BATCH_SIZE\n",
    "        num_batches += 1\n",
    "        loss_print_step = 200\n",
    "        if num_batches % loss_print_step == 0:\n",
    "            \n",
    "            print(f\"{num_batches} Average loss in the last {loss_print_step} batches is {loss_sum/float(loss_print_step)}\")\n",
    "            steps = 0\n",
    "            \n",
    "            \n",
    "            num_loss_prints += 1 \n",
    "            \n",
    "            if num_loss_prints % 10 == 0:\n",
    "                print_results(in_sentences, out_sentences, y_pred.to('cpu').detach().data, num_batches)\n",
    "            \n",
    "            if best_loss > loss_sum:\n",
    "                best_loss = loss_sum\n",
    "\n",
    "                models_path = \"models\"\n",
    "                if not os.path.exists(models_path):\n",
    "                    os.mkdir(models_path)\n",
    "\n",
    "                torch.save(rnn_encoder.state_dict(), trained_encoder_path)\n",
    "                torch.save(rnn_decoder.state_dict(), trained_decoder_path)\n",
    "            \n",
    "            loss_sum = 0\n",
    "            steps = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

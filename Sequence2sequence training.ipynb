{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sequence to Sequence Learning with Neural Networks* in PyTorch\n",
    "\n",
    "Encoder-decoder sequence to sequence RNN implementation based on 2014 publication:\n",
    "\n",
    "**Sequence to Sequence Learning with Neural Networks** - Ilya Sutskever, Oriol Vinyals, Quoc V. Le\n",
    "\n",
    "https://arxiv.org/abs/1409.3215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In the project files(PyTorch dataset implementation fra_eng_dataset.py) I use small toy dataset(170K sentences) in which can be found in the project files in fra-eng folder. But for this experiment I will try to use **WMT'14 English-German** dataset (4.5M sentences)\n",
    "\n",
    "The dataset can be found here:\n",
    "https://nlp.stanford.edu/projects/nmt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import os\n",
    "\n",
    "\n",
    "RNN_LAYERS = 4\n",
    "RNN_HIDDEN_SIZE = 1024\n",
    "IN_EMBEDDING_SIZE = 256\n",
    "OUT_EMBEDDING_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "MAXMAX_SENTENCE_LEN = 50\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_dictionary(text_corpus_path, top_n = 50000):\n",
    "\n",
    "    print(f\"Creating top dictionary from: {text_corpus_path}..\")\n",
    "    \n",
    "    last_token_idx = 0\n",
    "    token_dict = dict()\n",
    "    token_counts_list = []\n",
    "    token_idx_to_token = []\n",
    "    \n",
    "    with open(text_corpus_path, \"r\", encoding='utf-8') as f:\n",
    "        \n",
    "        for idx, line in enumerate(f.readlines()):\n",
    "            \n",
    "            if (idx+1)% 500000 == 0:\n",
    "                print(f\"Processed {idx+1} lines\")\n",
    "\n",
    "            line = line.replace('##AT##', '')\n",
    "            token_list = word_tokenize(line)\n",
    "            for token in token_list:\n",
    "                token = token.lower()\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = last_token_idx\n",
    "                    token_idx_to_token.append(token)\n",
    "                    token_counts_list.append((0,last_token_idx))\n",
    "                    last_token_idx += 1\n",
    "\n",
    "                token_idx = token_dict[token]\n",
    "                count, _ = token_counts_list[token_idx]\n",
    "                token_counts_list[token_idx] = (count+1,token_idx)\n",
    "                \n",
    "    token_counts_list = sorted(token_counts_list, reverse=True)\n",
    "    \n",
    "    top_token_list = []\n",
    "    \n",
    "    for idx, (count, token_idx) in enumerate(token_counts_list):\n",
    "        top_token_list.append(token_idx_to_token[token_idx])\n",
    "        \n",
    "        if idx > top_n:\n",
    "            break\n",
    "    \n",
    "    return top_token_list\n",
    "        \n",
    "\n",
    "\n",
    "class WMT14_en_de_Dataset(Dataset):\n",
    "    def __init__(self, data_source_path = 'wmt14_en_de'):\n",
    "        super().__init__()\n",
    "        \n",
    "        processed_data_path = \"processed_data.pkl\"\n",
    "        top_en_tokens_path = \"top_en_tokens.pkl\"\n",
    "        top_de_tokens_path = \"top_de_tokens.pkl\"\n",
    "        \n",
    "        self.sentence_list = []\n",
    "        \n",
    "        self.en_token_dict = dict()\n",
    "        self.en_token_dict['<PAD>'] = 0\n",
    "        self.en_token_dict['<EOS>'] = 1\n",
    "        self.en_token_dict['<UNK>'] = 2\n",
    "        self.en_last_token_idx = 2\n",
    "        self.en_token_idx_to_text = ['<PAD>', '<EOS>', '<UNK>']\n",
    "        \n",
    "        self.de_token_dict = dict()\n",
    "        self.de_token_dict['<PAD>'] = 0\n",
    "        self.de_token_dict['<EOS>'] = 1\n",
    "        self.de_token_dict['<UNK>'] = 2\n",
    "        self.de_last_token_idx = 2\n",
    "        self.de_token_idx_to_text = ['<PAD>', '<EOS>', '<UNK>']\n",
    "        \n",
    "        \n",
    "        if os.path.exists(processed_data_path):\n",
    "            with open(processed_data_path, 'rb') as f:\n",
    "                pickle_data = pickle.load(f)\n",
    "                self.sentence_list = pickle_data['sentence_list']\n",
    "                self.en_last_token_idx = pickle_data['en_last_token_idx']\n",
    "                self.de_last_token_idx = pickle_data['de_last_token_idx']\n",
    "                self.en_token_idx_to_text = pickle_data['en_token_idx_to_text']\n",
    "                self.de_token_idx_to_text = pickle_data['de_token_idx_to_text']\n",
    "        else:\n",
    "        \n",
    "            en_sentences_path = os.path.join(data_source_path, \"train.en\")\n",
    "            de_sentences_path = os.path.join(data_source_path, \"train.de\")\n",
    "            \n",
    "            if os.path.exists(top_en_tokens_path):\n",
    "                with open(top_en_tokens_path, \"rb\") as f:\n",
    "                    top_en_tokens = pickle.load(f)\n",
    "            else:\n",
    "                top_en_tokens = get_top_dictionary(en_sentences_path)\n",
    "                with open(top_en_tokens_path, \"wb\") as f:\n",
    "                    pickle.dump(top_en_tokens, f)\n",
    "            \n",
    "            \n",
    "            for token in top_en_tokens:\n",
    "                self.en_last_token_idx += 1\n",
    "                self.en_token_dict[token] = self.en_last_token_idx\n",
    "                self.en_token_idx_to_text.append(token)\n",
    "                \n",
    " \n",
    "            if os.path.exists(top_de_tokens_path):\n",
    "                with open(top_de_tokens_path, \"rb\") as f:\n",
    "                    top_de_tokens = pickle.load(f)\n",
    "            else:\n",
    "                top_de_tokens = get_top_dictionary(de_sentences_path)\n",
    "                with open(top_de_tokens_path, \"wb\") as f:\n",
    "                    pickle.dump(top_de_tokens, f)\n",
    "            \n",
    "            for token in top_de_tokens:\n",
    "                self.de_last_token_idx += 1\n",
    "                self.de_token_dict[token] = self.de_last_token_idx\n",
    "                self.de_token_idx_to_text.append(token)         \n",
    "                    \n",
    "            \n",
    "            with open(de_sentences_path, \"r\", encoding='utf-8') as de_f:\n",
    "                with open(en_sentences_path, \"r\", encoding='utf-8') as en_f:\n",
    "                    \n",
    "                    print(\"Creating sentences from {de_sentences_path} and {en_sentences_path} coropuses\")\n",
    "                    \n",
    "                    for idx, (de_sentence, en_sentence) in enumerate(zip(de_f.readlines(), en_f.readlines())):\n",
    "                        \n",
    "                        if (idx+1)%500000 == 0:\n",
    "                            print(f\"Processed {idx+1} lines\")\n",
    "                            \n",
    "                        de_sentence = de_sentence.replace('##AT##', '')\n",
    "                        en_sentence = en_sentence.replace('##AT##', '')\n",
    "                        \n",
    "                        en_token_sentence = []\n",
    "                        de_token_sentence = []\n",
    "\n",
    "                        en_token_list = word_tokenize(en_sentence)\n",
    "                        for token in en_token_list:\n",
    "                            token = token.lower()\n",
    "                            if token in self.en_token_dict:\n",
    "                                token_idx = self.en_token_dict[token]\n",
    "                            else:\n",
    "                                token_idx = self.en_token_dict['<UNK>']\n",
    "                                \n",
    "                            en_token_sentence.append(token_idx)\n",
    "\n",
    "                        en_token_sentence.append(self.en_token_dict['<EOS>'])\n",
    "\n",
    "                        de_token_list = word_tokenize(de_sentence)\n",
    "                        for token in de_token_list:\n",
    "                            token = token.lower()\n",
    "                            if token in self.de_token_dict:\n",
    "                                token_idx = self.de_token_dict[token]\n",
    "                            else:\n",
    "                                token_idx = self.de_token_dict['<UNK>']\n",
    "                                    \n",
    "\n",
    "                            de_token_sentence.append(token_idx)\n",
    "\n",
    "                        de_token_sentence.append(self.de_token_dict['<EOS>'])\n",
    "\n",
    "                        self.sentence_list.append(\n",
    "                            dict(\n",
    "                                en = en_token_sentence,\n",
    "                                de = de_token_sentence\n",
    "                            ))\n",
    "                        \n",
    "            with open(processed_data_path, \"wb\") as f:\n",
    "                pickle_processed_data = dict(\n",
    "                    sentence_list = self.sentence_list,\n",
    "                    en_last_token_idx = self.en_last_token_idx,\n",
    "                    de_last_token_idx = self.de_last_token_idx,\n",
    "                    en_token_idx_to_text = self.en_token_idx_to_text,\n",
    "                    de_token_idx_to_text = self.de_token_idx_to_text\n",
    "                )\n",
    "                pickle.dump(pickle_processed_data, f)\n",
    "            \n",
    "    def get_en_dict_size(self):\n",
    "        return self.en_last_token_idx + 1\n",
    "        \n",
    "    def get_de_dict_size(self):\n",
    "        return self.de_last_token_idx + 1\n",
    "    \n",
    "    def get_de_eos_code(self):\n",
    "        return self.de_token_dict['<EOS>']\n",
    "    \n",
    "    def get_en_eos_code(self):\n",
    "        return self.en_token_dict['<EOS>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        ret = dict()\n",
    "        for key in self.sentence_list[item]:\n",
    "            ret[key] = torch.tensor(self.sentence_list[item][key])\n",
    "        return ret\n",
    "\n",
    "\n",
    "def en_de_dataset_collate(data):\n",
    "\n",
    "    en_sentences = []\n",
    "    en_sentence_lens = []\n",
    "    de_sentences = []\n",
    "    de_sentence_lens = []\n",
    "    \n",
    "    en_sentences_sorted = []\n",
    "    en_sentence_lens_sorted = []\n",
    "    de_sentences_sorted = []\n",
    "    de_sentence_lens_sorted = []\n",
    "    \n",
    "    for s in data:\n",
    "        \n",
    "        sent = s['en'][0:MAXMAX_SENTENCE_LEN]\n",
    "        en_sentences.append(sent.unsqueeze(dim=1))\n",
    "        en_sentence_lens.append(len(sent))\n",
    "        \n",
    "        sent = s['de'][0:MAXMAX_SENTENCE_LEN]\n",
    "        de_sentences.append(sent.unsqueeze(dim=1))\n",
    "        de_sentence_lens.append(len(sent))\n",
    "\n",
    "    #Rearrange everything by de sentence lens\n",
    "    sort_idxes = np.argsort(np.array(de_sentence_lens))[::-1]\n",
    "    for idx in sort_idxes:\n",
    "        en_sentences_sorted.append(en_sentences[idx])\n",
    "        en_sentence_lens_sorted.append(en_sentence_lens[idx])\n",
    "        de_sentences_sorted.append(de_sentences[idx])\n",
    "        de_sentence_lens_sorted.append(de_sentence_lens[idx])\n",
    "    \n",
    "    return dict(\n",
    "        en_sentences = en_sentences_sorted,\n",
    "        en_lens = en_sentence_lens_sorted,\n",
    "        de_sentences = de_sentences_sorted,\n",
    "        de_lens = de_sentence_lens_sorted\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_encoder_model(nn.Module):\n",
    "    def __init__(self, in_dict_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dict_size = in_dict_size\n",
    "\n",
    "        self.embedding = nn.Linear(\n",
    "            in_dict_size, \n",
    "            IN_EMBEDDING_SIZE)\n",
    "        \n",
    "        \n",
    "        self.hidden = None \n",
    "        self.cell = None\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = IN_EMBEDDING_SIZE,\n",
    "            hidden_size = RNN_HIDDEN_SIZE,\n",
    "            num_layers = RNN_LAYERS\n",
    "        )\n",
    "        \n",
    "    def init_hidden_and_cell(self):\n",
    "        self.hidden = torch.randn(RNN_LAYERS, BATCH_SIZE, RNN_HIDDEN_SIZE).to(device)\n",
    "        self.cell = torch.rand(RNN_LAYERS, BATCH_SIZE, RNN_HIDDEN_SIZE).to(device)\n",
    "    \n",
    "    def get_hidden_and_cell(self):\n",
    "        return self.hidden, self.cell\n",
    "    \n",
    "    def forward(self, x):\n",
    "        padded_sent_one_hot, sent_lens = x\n",
    "        padded_sent_emb = self.embedding.forward(padded_sent_one_hot)\n",
    "        packed = pack_padded_sequence(padded_sent_emb, sent_lens)\n",
    "        packed, (self.hidden, self.cell) = self.rnn.forward(packed, (self.hidden,self.cell))\n",
    "        padded, sent_lens = pad_packed_sequence(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_decoder_model(nn.Module):\n",
    "    def __init__(self, out_dict_size):\n",
    "        super().__init__()\n",
    "      \n",
    "        self.in_embedding = nn.Linear(\n",
    "            in_features=out_dict_size,\n",
    "            out_features=IN_EMBEDDING_SIZE\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = IN_EMBEDDING_SIZE,\n",
    "            hidden_size = RNN_HIDDEN_SIZE,\n",
    "            num_layers = RNN_LAYERS\n",
    "        )\n",
    "\n",
    "        self.rnn_to_embedding = nn.Linear(\n",
    "            in_features = RNN_HIDDEN_SIZE,\n",
    "            out_features = OUT_EMBEDDING_SIZE\n",
    "        )\n",
    "\n",
    "        self.embedding_to_logit = nn.Linear(\n",
    "            in_features = OUT_EMBEDDING_SIZE, \n",
    "            out_features = out_dict_size\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def init_hidden_and_cell(self, hidden, cell):\n",
    "        self.hidden = hidden\n",
    "        self.cell = cell\n",
    "    \n",
    "    \n",
    "    def forward(self, out_eos_code, out_dict_size, max_sentence_len):\n",
    "        batch_size = self.hidden.shape[1]\n",
    "        prev_outp = (torch.ones(1, batch_size, 1) * out_eos_code).long()\n",
    "        prev_outp = prev_outp.to(device)\n",
    "        \n",
    "        all_outp_prob = []\n",
    "        \n",
    "        for timestep in range(max_sentence_len):\n",
    "            \n",
    "            prev_outp_one_hot = torch.zeros(prev_outp.shape[0], prev_outp.shape[1], out_dict_size).to(device)\n",
    "            prev_outp_one_hot = prev_outp_one_hot.scatter_(2,prev_outp.data,1)\n",
    "            \n",
    "            prev_outp_in_emb = self.in_embedding(prev_outp_one_hot)\n",
    "         \n",
    "            cur_outp_hid, (self.hidden, self.cell) = self.rnn.forward(prev_outp_in_emb, (self.hidden, self.cell))\n",
    "            cur_outp_emb = self.rnn_to_embedding.forward(cur_outp_hid)\n",
    "            cur_outp_logits = self.embedding_to_logit(cur_outp_emb)\n",
    "            cur_outp_prob = self.softmax(cur_outp_logits)\n",
    "            all_outp_prob.append(cur_outp_prob)\n",
    "            \n",
    "            prev_outp = torch.argmax(cur_outp_prob.detach().data.to(device), dim=2, keepdim=True)\n",
    "        \n",
    "        all_outp_prob_tensor = torch.cat(all_outp_prob, dim=0)\n",
    "    \n",
    "        return all_outp_prob_tensor\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WMT14_en_de_Dataset()\n",
    "sentences_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=en_de_dataset_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(in_sentence_list, out_sentence_list, pred_tensor, num_batches):\n",
    "    \n",
    "    sentence_prediction_samples_path = 'sentence_predictions.text'\n",
    "    \n",
    "    print(f\"Printing sample predictions after {num_batches} batches in {sentence_prediction_samples_path}\")\n",
    "    \n",
    "    with open(sentence_prediction_samples_path, \"a\") as f:\n",
    "\n",
    "        for i in range(3):\n",
    "            f.write('='*50 + '\\n')\n",
    "            \n",
    "        f.write(f\"Sample predictions after {num_batches} batches \\n\\n\")\n",
    "              \n",
    "        in_token_to_text = dataset.de_token_idx_to_text\n",
    "        out_token_to_text = dataset.en_token_idx_to_text\n",
    "\n",
    "        for s in range(min(len(in_sentence_list),50)):\n",
    "\n",
    "            in_sent_text = []\n",
    "            for in_token in in_sentence_list[s].squeeze():\n",
    "                in_sent_text.append(in_token_to_text[in_token])\n",
    "\n",
    "            f.write(f\"\\nGerman sentence is: {' '.join(in_sent_text)} \\n\")\n",
    "\n",
    "            out_sent_text = []\n",
    "\n",
    "            for out_token in out_sentence_list[s].squeeze():\n",
    "                  out_sent_text.append(out_token_to_text[out_token])\n",
    "            f.write(f\"English sentence is: {' '.join(out_sent_text)}\\n\")\n",
    "\n",
    "            pred_sent_text = []\n",
    "            for ts in range(pred_tensor.shape[0]):\n",
    "                pred_token = torch.argmax(pred_tensor[ts, s,:]).data\n",
    "                pred_sent_text.append(out_token_to_text[pred_token])\n",
    "\n",
    "                if pred_token == dataset.get_en_eos_code():\n",
    "                    break\n",
    "            f.write(f\"Translated English sentence is: {' '.join(pred_sent_text)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_encoder = RNN_encoder_model(dataset.get_de_dict_size()).to(device)\n",
    "rnn_decoder = RNN_decoder_model(dataset.get_en_dict_size()).to(device)\n",
    "\n",
    "trained_encoder_path = None\n",
    "trained_decoder_path = None\n",
    "\n",
    "trained_encoder_path = 'models/encoder_wmt14_de_en_2nd.pt'\n",
    "trained_decoder_path = 'models/decoder_wmt14_de_en_2nd.pt'\n",
    "\n",
    "if os.path.exists(trained_encoder_path):\n",
    "    rnn_encoder.load_state_dict(torch.load(trained_encoder_path))\n",
    "if os.path.exists(trained_decoder_path):\n",
    "    rnn_decoder.load_state_dict(torch.load(trained_decoder_path))\n",
    "\n",
    "\n",
    "params = list(rnn_encoder.parameters()) + list(rnn_decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 =====================\n",
      "200 Average loss in the last 200 batches is 10610.3486328125\n",
      "400 Average loss in the last 200 batches is 10613.1787109375\n",
      "600 Average loss in the last 200 batches is 10737.03515625\n",
      "800 Average loss in the last 200 batches is 10673.6484375\n",
      "1000 Average loss in the last 200 batches is 10596.572265625\n",
      "1200 Average loss in the last 200 batches is 10638.1064453125\n",
      "1400 Average loss in the last 200 batches is 10534.21484375\n",
      "1600 Average loss in the last 200 batches is 10520.798828125\n",
      "1800 Average loss in the last 200 batches is 10484.2138671875\n",
      "2000 Average loss in the last 200 batches is 10488.68359375\n",
      "Printing sample predictions after 2000 batches in sentence_predictions.text\n",
      "2200 Average loss in the last 200 batches is 10579.056640625\n",
      "2400 Average loss in the last 200 batches is 10500.4921875\n",
      "2600 Average loss in the last 200 batches is 10512.91015625\n",
      "2800 Average loss in the last 200 batches is 10415.119140625\n",
      "3000 Average loss in the last 200 batches is 10527.91015625\n",
      "3200 Average loss in the last 200 batches is 10515.162109375\n",
      "3400 Average loss in the last 200 batches is 10432.61328125\n",
      "3600 Average loss in the last 200 batches is 10486.447265625\n",
      "3800 Average loss in the last 200 batches is 10401.5615234375\n",
      "4000 Average loss in the last 200 batches is 10373.8232421875\n",
      "Printing sample predictions after 4000 batches in sentence_predictions.text\n",
      "4200 Average loss in the last 200 batches is 10323.4755859375\n",
      "4400 Average loss in the last 200 batches is 10381.01171875\n",
      "4600 Average loss in the last 200 batches is 10407.025390625\n",
      "4800 Average loss in the last 200 batches is 10374.5810546875\n",
      "5000 Average loss in the last 200 batches is 10416.3291015625\n",
      "5200 Average loss in the last 200 batches is 10310.4697265625\n",
      "5400 Average loss in the last 200 batches is 10300.63671875\n",
      "5600 Average loss in the last 200 batches is 10315.01171875\n",
      "5800 Average loss in the last 200 batches is 10340.9765625\n",
      "6000 Average loss in the last 200 batches is 10285.9248046875\n",
      "Printing sample predictions after 6000 batches in sentence_predictions.text\n",
      "6200 Average loss in the last 200 batches is 10243.287109375\n",
      "6400 Average loss in the last 200 batches is 10292.9921875\n",
      "6600 Average loss in the last 200 batches is 10319.6650390625\n",
      "6800 Average loss in the last 200 batches is 10203.646484375\n",
      "7000 Average loss in the last 200 batches is 10238.5146484375\n",
      "7200 Average loss in the last 200 batches is 10194.2841796875\n",
      "7400 Average loss in the last 200 batches is 10257.35546875\n",
      "7600 Average loss in the last 200 batches is 10254.8427734375\n",
      "7800 Average loss in the last 200 batches is 10208.9736328125\n",
      "8000 Average loss in the last 200 batches is 10183.974609375\n",
      "Printing sample predictions after 8000 batches in sentence_predictions.text\n",
      "8200 Average loss in the last 200 batches is 10171.361328125\n",
      "8400 Average loss in the last 200 batches is 10210.130859375\n",
      "8600 Average loss in the last 200 batches is 10110.36328125\n",
      "8800 Average loss in the last 200 batches is 10109.7490234375\n",
      "9000 Average loss in the last 200 batches is 10143.236328125\n",
      "9200 Average loss in the last 200 batches is 10151.9912109375\n",
      "9400 Average loss in the last 200 batches is 10210.0791015625\n",
      "9600 Average loss in the last 200 batches is 10043.5732421875\n",
      "9800 Average loss in the last 200 batches is 10106.75\n",
      "10000 Average loss in the last 200 batches is 10155.4677734375\n",
      "Printing sample predictions after 10000 batches in sentence_predictions.text\n",
      "10200 Average loss in the last 200 batches is 10162.7158203125\n",
      "10400 Average loss in the last 200 batches is 10046.755859375\n",
      "10600 Average loss in the last 200 batches is 10087.33984375\n",
      "10800 Average loss in the last 200 batches is 10113.767578125\n",
      "11000 Average loss in the last 200 batches is 10067.0830078125\n",
      "11200 Average loss in the last 200 batches is 10085.2421875\n",
      "11400 Average loss in the last 200 batches is 10066.19140625\n",
      "11600 Average loss in the last 200 batches is 10099.5380859375\n",
      "11800 Average loss in the last 200 batches is 9998.0810546875\n",
      "12000 Average loss in the last 200 batches is 10088.15234375\n",
      "Printing sample predictions after 12000 batches in sentence_predictions.text\n",
      "12200 Average loss in the last 200 batches is 10049.978515625\n",
      "12400 Average loss in the last 200 batches is 10040.13671875\n",
      "12600 Average loss in the last 200 batches is 10079.33203125\n",
      "12800 Average loss in the last 200 batches is 10047.345703125\n",
      "13000 Average loss in the last 200 batches is 10040.109375\n",
      "13200 Average loss in the last 200 batches is 9985.4248046875\n",
      "13400 Average loss in the last 200 batches is 10038.521484375\n",
      "13600 Average loss in the last 200 batches is 9983.8583984375\n",
      "13800 Average loss in the last 200 batches is 9931.04296875\n",
      "14000 Average loss in the last 200 batches is 9954.9658203125\n",
      "Printing sample predictions after 14000 batches in sentence_predictions.text\n",
      "14200 Average loss in the last 200 batches is 9920.8212890625\n",
      "14400 Average loss in the last 200 batches is 9966.31640625\n",
      "14600 Average loss in the last 200 batches is 9880.6064453125\n",
      "14800 Average loss in the last 200 batches is 9982.87890625\n",
      "15000 Average loss in the last 200 batches is 10009.3291015625\n",
      "15200 Average loss in the last 200 batches is 10076.603515625\n",
      "15400 Average loss in the last 200 batches is 9923.9658203125\n",
      "15600 Average loss in the last 200 batches is 9877.2216796875\n",
      "15800 Average loss in the last 200 batches is 9885.76953125\n",
      "16000 Average loss in the last 200 batches is 9956.9326171875\n",
      "Printing sample predictions after 16000 batches in sentence_predictions.text\n",
      "16200 Average loss in the last 200 batches is 9948.9345703125\n",
      "16400 Average loss in the last 200 batches is 9861.166015625\n",
      "16600 Average loss in the last 200 batches is 9943.12109375\n",
      "16800 Average loss in the last 200 batches is 9804.3076171875\n",
      "17000 Average loss in the last 200 batches is 9866.6875\n",
      "17200 Average loss in the last 200 batches is 9774.2373046875\n",
      "17400 Average loss in the last 200 batches is 9898.2060546875\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "num_batches = 0\n",
    "num_loss_prints = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"Starting epoch {epoch} =====================\")\n",
    "    \n",
    "    best_loss = 1e10\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for idx, sentences in enumerate(sentences_loader):\n",
    "\n",
    "        rnn_encoder.init_hidden_and_cell()\n",
    "       \n",
    "   \n",
    "        in_sentences = sentences['de_sentences']\n",
    "        in_lens = sentences['de_lens']\n",
    "        out_sentences = sentences['en_sentences']\n",
    "        out_lens = sentences['en_lens']\n",
    "        \n",
    "\n",
    "        padded_in = pad_sequence(in_sentences, padding_value=0).to(device)\n",
    "        padded_out = pad_sequence(out_sentences, padding_value=0).to(device)\n",
    "\n",
    "        padded_in_one_hot = torch.zeros(padded_in.shape[0], padded_in.shape[1], dataset.get_de_dict_size()).to(device)\n",
    "        padded_in_one_hot = padded_in_one_hot.scatter_(2,padded_in.data,1)\n",
    "       \n",
    "        rnn_encoder.forward((padded_in_one_hot, in_lens))\n",
    "        hidden, cell = rnn_encoder.get_hidden_and_cell()\n",
    "       \n",
    "        rnn_decoder.init_hidden_and_cell(hidden,cell)\n",
    "       \n",
    "        max_sentence_len = padded_out.shape[0]\n",
    "            \n",
    "        y_pred = rnn_decoder.forward(dataset.get_en_eos_code(), dataset.get_en_dict_size(), max_sentence_len)\n",
    "       \n",
    "        padded_out = padded_out[0:max_sentence_len]\n",
    "        padded_out_one_hot = torch.zeros(padded_out.shape[0], padded_out.shape[1], dataset.get_en_dict_size()).to(device)\n",
    "        padded_out_one_hot = padded_out_one_hot.scatter_(2,padded_out.data,1)\n",
    "       \n",
    "        #Make all padded one-hot vectors to all zeros, which will make\n",
    "        #padded components loss 0 and so they wont affect the loss\n",
    "        padded_out_one_hot[:,:,0] = torch.zeros(max_sentence_len, padded_out_one_hot.shape[1])\n",
    "        loss = torch.sum(-torch.log(y_pred + 1e-9) * padded_out_one_hot)\n",
    "       \n",
    "        loss_sum += loss.to('cpu').detach().data\n",
    "       \n",
    "        #print(loss.to('cpu').detach().data)\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Every 100 batches print the average loss and store the model weights\n",
    "        steps += BATCH_SIZE\n",
    "        num_batches += 1\n",
    "        loss_print_step = 200\n",
    "        if num_batches % loss_print_step == 0:\n",
    "            \n",
    "            print(f\"{num_batches} Average loss in the last {loss_print_step} batches is {loss_sum/float(loss_print_step)}\")\n",
    "            steps = 0\n",
    "            \n",
    "            \n",
    "            num_loss_prints += 1 \n",
    "            \n",
    "            if num_loss_prints % 10 == 0:\n",
    "                print_results(in_sentences, out_sentences, y_pred.to('cpu').detach().data, num_batches)\n",
    "            \n",
    "            if best_loss > loss_sum:\n",
    "                best_loss = loss_sum\n",
    "\n",
    "                models_path = \"models\"\n",
    "                if not os.path.exists(models_path):\n",
    "                    os.mkdir(models_path)\n",
    "\n",
    "                torch.save(rnn_encoder.state_dict(), trained_encoder_path)\n",
    "                torch.save(rnn_decoder.state_dict(), trained_decoder_path)\n",
    "            \n",
    "            loss_sum = 0\n",
    "            steps = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

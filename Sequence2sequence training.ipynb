{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sequence to Sequence Learning with Neural Networks* in PyTorch\n",
    "\n",
    "Encoder-decoder sequence to sequence RNN implementation based on 2014 publication:\n",
    "\n",
    "**Sequence to Sequence Learning with Neural Networks** - Ilya Sutskever, Oriol Vinyals, Quoc V. Le\n",
    "\n",
    "https://arxiv.org/abs/1409.3215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In the project files(PyTorch dataset implementation fra_eng_dataset.py) I use small toy dataset(170K sentences) in which can be found in the project files in fra-eng folder. But for this experiment I will try to use **WMT'14 English-German** dataset (4.5M sentences)\n",
    "\n",
    "The dataset can be found here:\n",
    "https://nlp.stanford.edu/projects/nmt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import os\n",
    "\n",
    "\n",
    "RNN_LAYERS = 4\n",
    "RNN_HIDDEN_SIZE = 1024\n",
    "IN_EMBEDDING_SIZE = 128\n",
    "OUT_EMBEDDING_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "MAXMAX_SENTENCE_LEN = 20\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_dictionary(text_corpus_path, top_n = 50000):\n",
    "\n",
    "    print(f\"Creating top dictionary from: {text_corpus_path}..\")\n",
    "    \n",
    "    last_token_idx = 0\n",
    "    token_dict = dict()\n",
    "    token_counts_list = []\n",
    "    token_idx_to_token = []\n",
    "    \n",
    "    with open(text_corpus_path, \"r\", encoding='utf-8') as f:\n",
    "        \n",
    "        for idx, line in enumerate(f.readlines()):\n",
    "            \n",
    "            if (idx+1)% 500000 == 0:\n",
    "                print(f\"Processed {idx+1} lines\")\n",
    "\n",
    "            line = line.replace('##AT##', '')\n",
    "            token_list = word_tokenize(line)\n",
    "            for token in token_list:\n",
    "                token = token.lower()\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = last_token_idx\n",
    "                    token_idx_to_token.append(token)\n",
    "                    token_counts_list.append((0,last_token_idx))\n",
    "                    last_token_idx += 1\n",
    "\n",
    "                token_idx = token_dict[token]\n",
    "                count, _ = token_counts_list[token_idx]\n",
    "                token_counts_list[token_idx] = (count+1,token_idx)\n",
    "                \n",
    "    token_counts_list = sorted(token_counts_list, reverse=True)\n",
    "    \n",
    "    top_token_list = []\n",
    "    \n",
    "    for idx, (count, token_idx) in enumerate(token_counts_list):\n",
    "        top_token_list.append(token_idx_to_token[token_idx])\n",
    "        \n",
    "        if idx > top_n:\n",
    "            break\n",
    "    \n",
    "    return top_token_list\n",
    "        \n",
    "\n",
    "\n",
    "class WMT14_en_de_Dataset(Dataset):\n",
    "    def __init__(self, data_source_path = 'wmt14_en_de'):\n",
    "        super().__init__()\n",
    "        \n",
    "        processed_data_path = \"processed_data.pkl\"\n",
    "        top_en_tokens_path = \"top_en_tokens.pkl\"\n",
    "        top_de_tokens_path = \"top_de_tokens.pkl\"\n",
    "        \n",
    "        self.sentence_list = []\n",
    "        \n",
    "        self.en_token_dict = dict()\n",
    "        self.en_token_dict['<PAD>'] = 0\n",
    "        self.en_token_dict['<EOS>'] = 1\n",
    "        self.en_token_dict['<UNK>'] = 2\n",
    "        self.en_last_token_idx = 2\n",
    "        self.en_token_idx_to_text = ['<PAD>', '<EOS>', '<UNK>']\n",
    "        \n",
    "        self.de_token_dict = dict()\n",
    "        self.de_token_dict['<PAD>'] = 0\n",
    "        self.de_token_dict['<EOS>'] = 1\n",
    "        self.de_token_dict['<UNK>'] = 2\n",
    "        self.de_last_token_idx = 2\n",
    "        self.de_token_idx_to_text = ['<PAD>', '<EOS>', '<UNK>']\n",
    "        \n",
    "        \n",
    "        if os.path.exists(processed_data_path):\n",
    "            with open(processed_data_path, 'rb') as f:\n",
    "                pickle_data = pickle.load(f)\n",
    "                self.sentence_list = pickle_data['sentence_list']\n",
    "                self.en_last_token_idx = pickle_data['en_last_token_idx']\n",
    "                self.de_last_token_idx = pickle_data['de_last_token_idx']\n",
    "                self.en_token_idx_to_text = pickle_data['en_token_idx_to_text']\n",
    "                self.de_token_idx_to_text = pickle_data['de_token_idx_to_text']\n",
    "        else:\n",
    "        \n",
    "            en_sentences_path = os.path.join(data_source_path, \"train.en\")\n",
    "            de_sentences_path = os.path.join(data_source_path, \"train.de\")\n",
    "            \n",
    "            if os.path.exists(top_en_tokens_path):\n",
    "                with open(top_en_tokens_path, \"rb\") as f:\n",
    "                    top_en_tokens = pickle.load(f)\n",
    "            else:\n",
    "                top_en_tokens = get_top_dictionary(en_sentences_path)\n",
    "                with open(top_en_tokens_path, \"wb\") as f:\n",
    "                    pickle.dump(top_en_tokens, f)\n",
    "            \n",
    "            \n",
    "            for token in top_en_tokens:\n",
    "                self.en_last_token_idx += 1\n",
    "                self.en_token_dict[token] = self.en_last_token_idx\n",
    "                self.en_token_idx_to_text.append(token)\n",
    "                \n",
    " \n",
    "            if os.path.exists(top_de_tokens_path):\n",
    "                with open(top_de_tokens_path, \"rb\") as f:\n",
    "                    top_de_tokens = pickle.load(f)\n",
    "            else:\n",
    "                top_de_tokens = get_top_dictionary(de_sentences_path)\n",
    "                with open(top_de_tokens_path, \"wb\") as f:\n",
    "                    pickle.dump(top_de_tokens, f)\n",
    "            \n",
    "            for token in top_de_tokens:\n",
    "                self.de_last_token_idx += 1\n",
    "                self.de_token_dict[token] = self.de_last_token_idx\n",
    "                self.de_token_idx_to_text.append(token)         \n",
    "                    \n",
    "            \n",
    "            with open(de_sentences_path, \"r\", encoding='utf-8') as de_f:\n",
    "                with open(en_sentences_path, \"r\", encoding='utf-8') as en_f:\n",
    "                    \n",
    "                    print(\"Creating sentences from {de_sentences_path} and {en_sentences_path} coropuses\")\n",
    "                    \n",
    "                    for idx, (de_sentence, en_sentence) in enumerate(zip(de_f.readlines(), en_f.readlines())):\n",
    "                        \n",
    "                        if (idx+1)%500000 == 0:\n",
    "                            print(f\"Processed {idx+1} lines\")\n",
    "                            \n",
    "                        de_sentence = de_sentence.replace('##AT##', '')\n",
    "                        en_sentence = en_sentence.replace('##AT##', '')\n",
    "                        \n",
    "                        en_token_sentence = []\n",
    "                        de_token_sentence = []\n",
    "\n",
    "                        en_token_list = word_tokenize(en_sentence)\n",
    "                        for token in en_token_list:\n",
    "                            token = token.lower()\n",
    "                            if token in self.en_token_dict:\n",
    "                                token_idx = self.en_token_dict[token]\n",
    "                            else:\n",
    "                                token_idx = self.en_token_dict['<UNK>']\n",
    "                                \n",
    "                            en_token_sentence.append(token_idx)\n",
    "\n",
    "                        en_token_sentence.append(self.en_token_dict['<EOS>'])\n",
    "\n",
    "                        de_token_list = word_tokenize(de_sentence)\n",
    "                        for token in de_token_list:\n",
    "                            token = token.lower()\n",
    "                            if token in self.de_token_dict:\n",
    "                                token_idx = self.de_token_dict[token]\n",
    "                            else:\n",
    "                                token_idx = self.de_token_dict['<UNK>']\n",
    "                                    \n",
    "\n",
    "                            de_token_sentence.append(token_idx)\n",
    "\n",
    "                        de_token_sentence.append(self.de_token_dict['<EOS>'])\n",
    "\n",
    "                        self.sentence_list.append(\n",
    "                            dict(\n",
    "                                en = en_token_sentence,\n",
    "                                de = de_token_sentence\n",
    "                            ))\n",
    "                        \n",
    "            with open(processed_data_path, \"wb\") as f:\n",
    "                pickle_processed_data = dict(\n",
    "                    sentence_list = self.sentence_list,\n",
    "                    en_last_token_idx = self.en_last_token_idx,\n",
    "                    de_last_token_idx = self.de_last_token_idx,\n",
    "                    en_token_idx_to_text = self.en_token_idx_to_text,\n",
    "                    de_token_idx_to_text = self.de_token_idx_to_text\n",
    "                )\n",
    "                pickle.dump(pickle_processed_data, f)\n",
    "            \n",
    "    def get_en_dict_size(self):\n",
    "        return self.en_last_token_idx + 1\n",
    "        \n",
    "    def get_de_dict_size(self):\n",
    "        return self.de_last_token_idx + 1\n",
    "    \n",
    "    def get_de_eos_code(self):\n",
    "        return self.de_token_dict['<EOS>']\n",
    "    \n",
    "    def get_en_eos_code(self):\n",
    "        return self.en_token_dict['<EOS>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        ret = dict()\n",
    "        for key in self.sentence_list[item]:\n",
    "            ret[key] = torch.tensor(self.sentence_list[item][key])\n",
    "        return ret\n",
    "\n",
    "\n",
    "def en_de_dataset_collate(data):\n",
    "\n",
    "    en_sentences = []\n",
    "    en_sentence_lens = []\n",
    "    de_sentences = []\n",
    "    de_sentence_lens = []\n",
    "    \n",
    "    en_sentences_sorted = []\n",
    "    en_sentence_lens_sorted = []\n",
    "    de_sentences_sorted = []\n",
    "    de_sentence_lens_sorted = []\n",
    "    \n",
    "    for s in data:\n",
    "        \n",
    "        sent = s['en'][0:MAXMAX_SENTENCE_LEN]\n",
    "        en_sentences.append(sent.unsqueeze(dim=1))\n",
    "        en_sentence_lens.append(len(sent))\n",
    "        \n",
    "        sent = s['de'][0:MAXMAX_SENTENCE_LEN]\n",
    "        de_sentences.append(sent.unsqueeze(dim=1))\n",
    "        de_sentence_lens.append(len(sent))\n",
    "\n",
    "    #Rearrange everything by de sentence lens\n",
    "    sort_idxes = np.argsort(np.array(de_sentence_lens))[::-1]\n",
    "    for idx in sort_idxes:\n",
    "        en_sentences_sorted.append(en_sentences[idx])\n",
    "        en_sentence_lens_sorted.append(en_sentence_lens[idx])\n",
    "        de_sentences_sorted.append(de_sentences[idx])\n",
    "        de_sentence_lens_sorted.append(de_sentence_lens[idx])\n",
    "    \n",
    "    return dict(\n",
    "        en_sentences = en_sentences_sorted,\n",
    "        en_lens = en_sentence_lens_sorted,\n",
    "        de_sentences = de_sentences_sorted,\n",
    "        de_lens = de_sentence_lens_sorted\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_encoder_model(nn.Module):\n",
    "    def __init__(self, in_dict_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dict_size = in_dict_size\n",
    "\n",
    "        self.embedding = nn.Linear(\n",
    "            in_dict_size, \n",
    "            IN_EMBEDDING_SIZE)\n",
    "        \n",
    "        \n",
    "        self.hidden = None \n",
    "        self.cell = None\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = IN_EMBEDDING_SIZE,\n",
    "            hidden_size = RNN_HIDDEN_SIZE,\n",
    "            num_layers = RNN_LAYERS\n",
    "        )\n",
    "        \n",
    "    def init_hidden_and_cell(self):\n",
    "        self.hidden = torch.randn(RNN_LAYERS, BATCH_SIZE, RNN_HIDDEN_SIZE).to(device)\n",
    "        self.cell = torch.rand(RNN_LAYERS, BATCH_SIZE, RNN_HIDDEN_SIZE).to(device)\n",
    "    \n",
    "    def get_hidden_and_cell(self):\n",
    "        return self.hidden, self.cell\n",
    "    \n",
    "    def forward(self, x):\n",
    "        padded_sent_one_hot, sent_lens = x\n",
    "        padded_sent_emb = self.embedding.forward(padded_sent_one_hot)\n",
    "        packed = pack_padded_sequence(padded_sent_emb, sent_lens)\n",
    "        packed, (self.hidden, self.cell) = self.rnn.forward(packed, (self.hidden,self.cell))\n",
    "        padded, sent_lens = pad_packed_sequence(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_decoder_model(nn.Module):\n",
    "    def __init__(self, out_dict_size):\n",
    "        super().__init__()\n",
    "      \n",
    "        self.in_embedding = nn.Linear(\n",
    "            in_features=out_dict_size,\n",
    "            out_features=IN_EMBEDDING_SIZE\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = IN_EMBEDDING_SIZE,\n",
    "            hidden_size = RNN_HIDDEN_SIZE,\n",
    "            num_layers = RNN_LAYERS\n",
    "        )\n",
    "\n",
    "        self.rnn_to_embedding = nn.Linear(\n",
    "            in_features = RNN_HIDDEN_SIZE,\n",
    "            out_features = OUT_EMBEDDING_SIZE\n",
    "        )\n",
    "\n",
    "        self.embedding_to_logit = nn.Linear(\n",
    "            in_features = OUT_EMBEDDING_SIZE, \n",
    "            out_features = out_dict_size\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def init_hidden_and_cell(self, hidden, cell):\n",
    "        self.hidden = hidden\n",
    "        self.cell = cell\n",
    "    \n",
    "    \n",
    "    def forward(self, out_eos_code, out_dict_size, max_sentence_len):\n",
    "        batch_size = self.hidden.shape[1]\n",
    "        prev_outp = (torch.ones(1, batch_size, 1) * out_eos_code).long()\n",
    "        prev_outp = prev_outp.to(device)\n",
    "        \n",
    "        all_outp_prob = []\n",
    "        \n",
    "        for timestep in range(max_sentence_len):\n",
    "            \n",
    "            prev_outp_one_hot = torch.zeros(prev_outp.shape[0], prev_outp.shape[1], out_dict_size).to(device)\n",
    "            prev_outp_one_hot = prev_outp_one_hot.scatter_(2,prev_outp.data,1)\n",
    "            \n",
    "            prev_outp_in_emb = self.in_embedding(prev_outp_one_hot)\n",
    "         \n",
    "            cur_outp_hid, (self.hidden, self.cell) = self.rnn.forward(prev_outp_in_emb, (self.hidden, self.cell))\n",
    "            cur_outp_emb = self.rnn_to_embedding.forward(cur_outp_hid)\n",
    "            cur_outp_logits = self.embedding_to_logit(cur_outp_emb)\n",
    "            cur_outp_prob = self.softmax(cur_outp_logits)\n",
    "            all_outp_prob.append(cur_outp_prob)\n",
    "            \n",
    "            prev_outp = torch.argmax(cur_outp_prob.data.to(device), dim=2, keepdim=True)\n",
    "        \n",
    "        all_outp_prob_tensor = torch.cat(all_outp_prob, dim=0)\n",
    "    \n",
    "        return all_outp_prob_tensor\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WMT14_en_de_Dataset()\n",
    "sentences_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=en_de_dataset_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(in_sentence_list, out_sentence_list, pred_tensor, num_batches):\n",
    "    \n",
    "    sentence_prediction_samples_path = 'sentence_predictions.text'\n",
    "    \n",
    "    print(f\"Printing sample predictions after {num_batches} batches in {sentence_prediction_samples_path}\")\n",
    "    \n",
    "    with open(sentence_prediction_samples_path, \"a\") as f:\n",
    "\n",
    "        for i in range(3):\n",
    "            f.write('='*50 + '\\n')\n",
    "            \n",
    "        f.write(f\"Sample predictions after {num_batches} batches \\n\\n\")\n",
    "              \n",
    "        in_token_to_text = dataset.de_token_idx_to_text\n",
    "        out_token_to_text = dataset.en_token_idx_to_text\n",
    "\n",
    "        for s in range(min(len(in_sentence_list),50)):\n",
    "\n",
    "            in_sent_text = []\n",
    "            for in_token in in_sentence_list[s].squeeze():\n",
    "                in_sent_text.append(in_token_to_text[in_token])\n",
    "\n",
    "            f.write(f\"\\nGerman sentence is: {' '.join(in_sent_text)} \\n\")\n",
    "\n",
    "            out_sent_text = []\n",
    "\n",
    "            for out_token in out_sentence_list[s].squeeze():\n",
    "                  out_sent_text.append(out_token_to_text[out_token])\n",
    "            f.write(f\"English sentence is: {' '.join(out_sent_text)}\\n\")\n",
    "\n",
    "            pred_sent_text = []\n",
    "            for ts in range(pred_tensor.shape[0]):\n",
    "                pred_token = torch.argmax(pred_tensor[ts, s,:]).data\n",
    "                pred_sent_text.append(out_token_to_text[pred_token])\n",
    "\n",
    "                if pred_token == dataset.get_en_eos_code():\n",
    "                    break\n",
    "            f.write(f\"Translated English sentence is: {' '.join(pred_sent_text)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_encoder = RNN_encoder_model(dataset.get_de_dict_size()).to(device)\n",
    "rnn_decoder = RNN_decoder_model(dataset.get_en_dict_size()).to(device)\n",
    "\n",
    "trained_encoder_path = None\n",
    "trained_decoder_path = None\n",
    "\n",
    "trained_encoder_path = 'models/encoder_wmt14_de_en.pt'\n",
    "trained_decoder_path = 'models/decoder_wmt14_de_en.pt'\n",
    "\n",
    "if os.path.exists(trained_encoder_path):\n",
    "    rnn_encoder.load_state_dict(torch.load(trained_encoder_path))\n",
    "if os.path.exists(trained_decoder_path):\n",
    "    rnn_decoder.load_state_dict(torch.load(trained_decoder_path))\n",
    "\n",
    "\n",
    "params = list(rnn_encoder.parameters()) + list(rnn_decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 =====================\n",
      "200 Average loss in the last 200 batches is 15070.2412109375\n",
      "400 Average loss in the last 200 batches is 14894.48828125\n",
      "600 Average loss in the last 200 batches is 14763.2783203125\n",
      "800 Average loss in the last 200 batches is 14697.6083984375\n",
      "1000 Average loss in the last 200 batches is 14628.3720703125\n",
      "1200 Average loss in the last 200 batches is 14548.44921875\n",
      "1400 Average loss in the last 200 batches is 14447.9462890625\n",
      "1600 Average loss in the last 200 batches is 14417.166015625\n",
      "1800 Average loss in the last 200 batches is 14295.33203125\n",
      "2000 Average loss in the last 200 batches is 14311.87109375\n",
      "Printing sample predictions after 2000 batches in sentence_predictions.text\n",
      "2200 Average loss in the last 200 batches is 14228.9013671875\n",
      "2400 Average loss in the last 200 batches is 14185.505859375\n",
      "2600 Average loss in the last 200 batches is 14131.49609375\n",
      "2800 Average loss in the last 200 batches is 14131.35546875\n",
      "3000 Average loss in the last 200 batches is 14102.1298828125\n",
      "3200 Average loss in the last 200 batches is 14002.0009765625\n",
      "3400 Average loss in the last 200 batches is 13970.92578125\n",
      "3600 Average loss in the last 200 batches is 13859.416015625\n",
      "3800 Average loss in the last 200 batches is 13848.23046875\n",
      "4000 Average loss in the last 200 batches is 13818.4150390625\n",
      "Printing sample predictions after 4000 batches in sentence_predictions.text\n",
      "4200 Average loss in the last 200 batches is 13837.005859375\n",
      "4400 Average loss in the last 200 batches is 13749.712890625\n",
      "4600 Average loss in the last 200 batches is 13668.41796875\n",
      "4800 Average loss in the last 200 batches is 13670.275390625\n",
      "5000 Average loss in the last 200 batches is 13591.1171875\n",
      "5200 Average loss in the last 200 batches is 13566.8896484375\n",
      "5400 Average loss in the last 200 batches is 13574.1015625\n",
      "5600 Average loss in the last 200 batches is 13535.5009765625\n",
      "5800 Average loss in the last 200 batches is 13497.0947265625\n",
      "6000 Average loss in the last 200 batches is 13489.90625\n",
      "Printing sample predictions after 6000 batches in sentence_predictions.text\n",
      "6200 Average loss in the last 200 batches is 13419.607421875\n",
      "6400 Average loss in the last 200 batches is 13431.134765625\n",
      "6600 Average loss in the last 200 batches is 13382.369140625\n",
      "6800 Average loss in the last 200 batches is 13366.490234375\n",
      "7000 Average loss in the last 200 batches is 13344.4345703125\n",
      "7200 Average loss in the last 200 batches is 13284.025390625\n",
      "7400 Average loss in the last 200 batches is 13317.83984375\n",
      "7600 Average loss in the last 200 batches is 13259.5029296875\n",
      "7800 Average loss in the last 200 batches is 13226.2138671875\n",
      "8000 Average loss in the last 200 batches is 13204.005859375\n",
      "Printing sample predictions after 8000 batches in sentence_predictions.text\n",
      "8200 Average loss in the last 200 batches is 13174.76171875\n",
      "8400 Average loss in the last 200 batches is 13136.51171875\n",
      "8600 Average loss in the last 200 batches is 13140.337890625\n",
      "8800 Average loss in the last 200 batches is 13126.94140625\n",
      "9000 Average loss in the last 200 batches is 13055.1328125\n",
      "9200 Average loss in the last 200 batches is 13040.318359375\n",
      "9400 Average loss in the last 200 batches is 13045.068359375\n",
      "9600 Average loss in the last 200 batches is 13053.724609375\n",
      "9800 Average loss in the last 200 batches is 13026.0859375\n",
      "10000 Average loss in the last 200 batches is 12989.9521484375\n",
      "Printing sample predictions after 10000 batches in sentence_predictions.text\n",
      "10200 Average loss in the last 200 batches is 12969.65625\n",
      "10400 Average loss in the last 200 batches is 12924.568359375\n",
      "10600 Average loss in the last 200 batches is 12936.4990234375\n",
      "10800 Average loss in the last 200 batches is 12865.7001953125\n",
      "11000 Average loss in the last 200 batches is 12847.3251953125\n",
      "11200 Average loss in the last 200 batches is 12840.6171875\n",
      "11400 Average loss in the last 200 batches is 12838.8427734375\n",
      "11600 Average loss in the last 200 batches is 12806.7412109375\n",
      "11800 Average loss in the last 200 batches is 12777.7802734375\n",
      "12000 Average loss in the last 200 batches is 12768.1904296875\n",
      "Printing sample predictions after 12000 batches in sentence_predictions.text\n",
      "12200 Average loss in the last 200 batches is 12784.490234375\n",
      "12400 Average loss in the last 200 batches is 12725.4873046875\n",
      "12600 Average loss in the last 200 batches is 12705.9013671875\n",
      "12800 Average loss in the last 200 batches is 12729.021484375\n",
      "13000 Average loss in the last 200 batches is 12680.712890625\n",
      "13200 Average loss in the last 200 batches is 12659.47265625\n",
      "13400 Average loss in the last 200 batches is 12617.7890625\n",
      "13600 Average loss in the last 200 batches is 12630.712890625\n",
      "13800 Average loss in the last 200 batches is 12586.7197265625\n",
      "14000 Average loss in the last 200 batches is 12615.494140625\n",
      "Printing sample predictions after 14000 batches in sentence_predictions.text\n",
      "14200 Average loss in the last 200 batches is 12617.052734375\n",
      "14400 Average loss in the last 200 batches is 12598.51171875\n",
      "14600 Average loss in the last 200 batches is 12581.708984375\n",
      "14800 Average loss in the last 200 batches is 12551.0009765625\n",
      "15000 Average loss in the last 200 batches is 12530.7373046875\n",
      "15200 Average loss in the last 200 batches is 12495.93359375\n",
      "15400 Average loss in the last 200 batches is 12479.4765625\n",
      "15600 Average loss in the last 200 batches is 12456.2509765625\n",
      "15800 Average loss in the last 200 batches is 12403.982421875\n",
      "16000 Average loss in the last 200 batches is 12466.369140625\n",
      "Printing sample predictions after 16000 batches in sentence_predictions.text\n",
      "16200 Average loss in the last 200 batches is 12415.1904296875\n",
      "16400 Average loss in the last 200 batches is 12392.5400390625\n",
      "16600 Average loss in the last 200 batches is 12392.0361328125\n",
      "16800 Average loss in the last 200 batches is 12321.048828125\n",
      "17000 Average loss in the last 200 batches is 12328.6337890625\n",
      "17200 Average loss in the last 200 batches is 12317.33984375\n",
      "17400 Average loss in the last 200 batches is 12370.6650390625\n",
      "17600 Average loss in the last 200 batches is 12326.8935546875\n",
      "17800 Average loss in the last 200 batches is 12278.7841796875\n",
      "18000 Average loss in the last 200 batches is 12300.2421875\n",
      "Printing sample predictions after 18000 batches in sentence_predictions.text\n",
      "18200 Average loss in the last 200 batches is 12244.3837890625\n",
      "18400 Average loss in the last 200 batches is 12320.8427734375\n",
      "18600 Average loss in the last 200 batches is 12271.40234375\n",
      "18800 Average loss in the last 200 batches is 12235.1865234375\n",
      "19000 Average loss in the last 200 batches is 12268.7158203125\n",
      "19200 Average loss in the last 200 batches is 12228.982421875\n",
      "19400 Average loss in the last 200 batches is 12167.607421875\n",
      "19600 Average loss in the last 200 batches is 12176.2958984375\n",
      "19800 Average loss in the last 200 batches is 12190.6337890625\n",
      "20000 Average loss in the last 200 batches is 12193.1650390625\n",
      "Printing sample predictions after 20000 batches in sentence_predictions.text\n",
      "20200 Average loss in the last 200 batches is 12141.3671875\n",
      "20400 Average loss in the last 200 batches is 12124.5302734375\n",
      "20600 Average loss in the last 200 batches is 12138.0146484375\n",
      "20800 Average loss in the last 200 batches is 12123.92578125\n",
      "21000 Average loss in the last 200 batches is 12135.69921875\n",
      "21200 Average loss in the last 200 batches is 12103.080078125\n",
      "21400 Average loss in the last 200 batches is 12010.21484375\n",
      "21600 Average loss in the last 200 batches is 12092.02734375\n",
      "21800 Average loss in the last 200 batches is 12064.2578125\n",
      "22000 Average loss in the last 200 batches is 12057.6484375\n",
      "Printing sample predictions after 22000 batches in sentence_predictions.text\n",
      "22200 Average loss in the last 200 batches is 12042.4365234375\n",
      "22400 Average loss in the last 200 batches is 12059.0185546875\n",
      "22600 Average loss in the last 200 batches is 12011.7216796875\n",
      "22800 Average loss in the last 200 batches is 12008.8515625\n",
      "23000 Average loss in the last 200 batches is 11991.1728515625\n",
      "23200 Average loss in the last 200 batches is 12010.94140625\n",
      "23400 Average loss in the last 200 batches is 11988.478515625\n",
      "23600 Average loss in the last 200 batches is 11991.74609375\n",
      "23800 Average loss in the last 200 batches is 11957.830078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 Average loss in the last 200 batches is 11962.40234375\n",
      "Printing sample predictions after 24000 batches in sentence_predictions.text\n",
      "24200 Average loss in the last 200 batches is 11963.759765625\n",
      "24400 Average loss in the last 200 batches is 11932.4873046875\n",
      "24600 Average loss in the last 200 batches is 11936.9072265625\n",
      "24800 Average loss in the last 200 batches is 11911.490234375\n",
      "25000 Average loss in the last 200 batches is 11897.30078125\n",
      "25200 Average loss in the last 200 batches is 11898.2958984375\n",
      "25400 Average loss in the last 200 batches is 11848.021484375\n",
      "25600 Average loss in the last 200 batches is 11850.505859375\n",
      "25800 Average loss in the last 200 batches is 11899.7265625\n",
      "26000 Average loss in the last 200 batches is 11879.7822265625\n",
      "Printing sample predictions after 26000 batches in sentence_predictions.text\n",
      "26200 Average loss in the last 200 batches is 11845.486328125\n",
      "26400 Average loss in the last 200 batches is 11820.9140625\n",
      "26600 Average loss in the last 200 batches is 11818.88671875\n",
      "26800 Average loss in the last 200 batches is 11784.2333984375\n",
      "27000 Average loss in the last 200 batches is 11828.4111328125\n",
      "27200 Average loss in the last 200 batches is 11813.712890625\n",
      "27400 Average loss in the last 200 batches is 11796.962890625\n",
      "27600 Average loss in the last 200 batches is 11813.0078125\n",
      "27800 Average loss in the last 200 batches is 11782.818359375\n",
      "28000 Average loss in the last 200 batches is 11739.6201171875\n",
      "Printing sample predictions after 28000 batches in sentence_predictions.text\n",
      "28200 Average loss in the last 200 batches is 11744.8037109375\n",
      "28400 Average loss in the last 200 batches is 11752.375\n",
      "28600 Average loss in the last 200 batches is 11720.75\n",
      "28800 Average loss in the last 200 batches is 11740.580078125\n",
      "29000 Average loss in the last 200 batches is 11705.83984375\n",
      "29200 Average loss in the last 200 batches is 11700.2783203125\n",
      "29400 Average loss in the last 200 batches is 11731.056640625\n",
      "29600 Average loss in the last 200 batches is 11674.1484375\n",
      "29800 Average loss in the last 200 batches is 11720.5546875\n",
      "30000 Average loss in the last 200 batches is 11663.7314453125\n",
      "Printing sample predictions after 30000 batches in sentence_predictions.text\n",
      "30200 Average loss in the last 200 batches is 11683.6396484375\n",
      "30400 Average loss in the last 200 batches is 11688.8212890625\n",
      "30600 Average loss in the last 200 batches is 11625.3291015625\n",
      "30800 Average loss in the last 200 batches is 11687.2451171875\n",
      "31000 Average loss in the last 200 batches is 11639.6376953125\n",
      "31200 Average loss in the last 200 batches is 11642.990234375\n",
      "31400 Average loss in the last 200 batches is 11611.0166015625\n",
      "31600 Average loss in the last 200 batches is 11604.896484375\n",
      "31800 Average loss in the last 200 batches is 11630.4189453125\n",
      "32000 Average loss in the last 200 batches is 11622.271484375\n",
      "Printing sample predictions after 32000 batches in sentence_predictions.text\n",
      "32200 Average loss in the last 200 batches is 11606.5146484375\n",
      "32400 Average loss in the last 200 batches is 11569.8076171875\n",
      "32600 Average loss in the last 200 batches is 11590.7138671875\n",
      "32800 Average loss in the last 200 batches is 11621.4150390625\n",
      "33000 Average loss in the last 200 batches is 11598.1845703125\n",
      "33200 Average loss in the last 200 batches is 11564.9501953125\n",
      "33400 Average loss in the last 200 batches is 11566.103515625\n",
      "33600 Average loss in the last 200 batches is 11529.64453125\n",
      "33800 Average loss in the last 200 batches is 11554.23828125\n",
      "34000 Average loss in the last 200 batches is 11510.24609375\n",
      "Printing sample predictions after 34000 batches in sentence_predictions.text\n",
      "34200 Average loss in the last 200 batches is 11524.2353515625\n",
      "34400 Average loss in the last 200 batches is 11538.49609375\n",
      "34600 Average loss in the last 200 batches is 11553.259765625\n",
      "34800 Average loss in the last 200 batches is 11481.4326171875\n",
      "Starting epoch 1 =====================\n",
      "35000 Average loss in the last 200 batches is 5008.12109375\n",
      "35200 Average loss in the last 200 batches is 11400.548828125\n",
      "35400 Average loss in the last 200 batches is 11443.8154296875\n",
      "35600 Average loss in the last 200 batches is 11356.369140625\n",
      "35800 Average loss in the last 200 batches is 11393.7822265625\n",
      "36000 Average loss in the last 200 batches is 11448.208984375\n",
      "Printing sample predictions after 36000 batches in sentence_predictions.text\n",
      "36200 Average loss in the last 200 batches is 11432.3447265625\n",
      "36400 Average loss in the last 200 batches is 11433.412109375\n",
      "36600 Average loss in the last 200 batches is 11428.20703125\n",
      "36800 Average loss in the last 200 batches is 11429.642578125\n",
      "37000 Average loss in the last 200 batches is 11392.26953125\n",
      "37200 Average loss in the last 200 batches is 11300.5576171875\n",
      "37400 Average loss in the last 200 batches is 11351.197265625\n",
      "37600 Average loss in the last 200 batches is 11373.521484375\n",
      "37800 Average loss in the last 200 batches is 11391.97265625\n",
      "38000 Average loss in the last 200 batches is 11381.3173828125\n",
      "Printing sample predictions after 38000 batches in sentence_predictions.text\n",
      "38200 Average loss in the last 200 batches is 11365.427734375\n",
      "38400 Average loss in the last 200 batches is 11374.1171875\n",
      "38600 Average loss in the last 200 batches is 11374.6083984375\n",
      "38800 Average loss in the last 200 batches is 11377.7685546875\n",
      "39000 Average loss in the last 200 batches is 11362.068359375\n",
      "39200 Average loss in the last 200 batches is 11372.646484375\n",
      "39400 Average loss in the last 200 batches is 11370.91015625\n",
      "39600 Average loss in the last 200 batches is 11347.77734375\n",
      "39800 Average loss in the last 200 batches is 11357.2353515625\n",
      "40000 Average loss in the last 200 batches is 11347.7060546875\n",
      "Printing sample predictions after 40000 batches in sentence_predictions.text\n",
      "40200 Average loss in the last 200 batches is 11329.5078125\n",
      "40400 Average loss in the last 200 batches is 11363.9921875\n",
      "40600 Average loss in the last 200 batches is 11294.8984375\n",
      "40800 Average loss in the last 200 batches is 11353.5146484375\n",
      "41000 Average loss in the last 200 batches is 11349.9912109375\n",
      "41200 Average loss in the last 200 batches is 11359.0595703125\n",
      "41400 Average loss in the last 200 batches is 11296.927734375\n",
      "41600 Average loss in the last 200 batches is 11310.3388671875\n",
      "41800 Average loss in the last 200 batches is 11257.462890625\n",
      "42000 Average loss in the last 200 batches is 11301.8740234375\n",
      "Printing sample predictions after 42000 batches in sentence_predictions.text\n",
      "42200 Average loss in the last 200 batches is 11298.3037109375\n",
      "42400 Average loss in the last 200 batches is 11306.71484375\n",
      "42600 Average loss in the last 200 batches is 11303.912109375\n",
      "42800 Average loss in the last 200 batches is 11322.4501953125\n",
      "43000 Average loss in the last 200 batches is 11300.8623046875\n",
      "43200 Average loss in the last 200 batches is 11325.4326171875\n",
      "43400 Average loss in the last 200 batches is 11289.2861328125\n",
      "43600 Average loss in the last 200 batches is 11274.46875\n",
      "43800 Average loss in the last 200 batches is 11229.34375\n",
      "44000 Average loss in the last 200 batches is 11266.96875\n",
      "Printing sample predictions after 44000 batches in sentence_predictions.text\n",
      "44200 Average loss in the last 200 batches is 11287.8134765625\n",
      "44400 Average loss in the last 200 batches is 11230.86328125\n",
      "44600 Average loss in the last 200 batches is 11234.888671875\n",
      "44800 Average loss in the last 200 batches is 11209.64453125\n",
      "45000 Average loss in the last 200 batches is 11252.8583984375\n",
      "45200 Average loss in the last 200 batches is 11224.2666015625\n",
      "45400 Average loss in the last 200 batches is 11229.0234375\n",
      "45600 Average loss in the last 200 batches is 11253.08203125\n",
      "45800 Average loss in the last 200 batches is 11199.7685546875\n",
      "46000 Average loss in the last 200 batches is 11178.4521484375\n",
      "Printing sample predictions after 46000 batches in sentence_predictions.text\n",
      "46200 Average loss in the last 200 batches is 11213.7578125\n",
      "46400 Average loss in the last 200 batches is 11236.822265625\n",
      "46600 Average loss in the last 200 batches is 11229.302734375\n",
      "46800 Average loss in the last 200 batches is 11198.3486328125\n",
      "47000 Average loss in the last 200 batches is 11208.986328125\n",
      "47200 Average loss in the last 200 batches is 11179.083984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47400 Average loss in the last 200 batches is 11190.5966796875\n",
      "47600 Average loss in the last 200 batches is 11204.7666015625\n",
      "47800 Average loss in the last 200 batches is 11151.2060546875\n",
      "48000 Average loss in the last 200 batches is 11186.5341796875\n",
      "Printing sample predictions after 48000 batches in sentence_predictions.text\n",
      "48200 Average loss in the last 200 batches is 11207.580078125\n",
      "48400 Average loss in the last 200 batches is 11133.84375\n",
      "48600 Average loss in the last 200 batches is 11160.302734375\n",
      "48800 Average loss in the last 200 batches is 11178.63671875\n",
      "49000 Average loss in the last 200 batches is 11151.1533203125\n",
      "49200 Average loss in the last 200 batches is 11166.1298828125\n",
      "49400 Average loss in the last 200 batches is 11207.677734375\n",
      "49600 Average loss in the last 200 batches is 11086.244140625\n",
      "49800 Average loss in the last 200 batches is 11183.81640625\n",
      "50000 Average loss in the last 200 batches is 11185.8515625\n",
      "Printing sample predictions after 50000 batches in sentence_predictions.text\n",
      "50200 Average loss in the last 200 batches is 11144.9921875\n",
      "50400 Average loss in the last 200 batches is 11143.244140625\n",
      "50600 Average loss in the last 200 batches is 11106.134765625\n",
      "50800 Average loss in the last 200 batches is 11140.255859375\n",
      "51000 Average loss in the last 200 batches is 11167.1123046875\n",
      "51200 Average loss in the last 200 batches is 11168.8388671875\n",
      "51400 Average loss in the last 200 batches is 11188.3515625\n",
      "51600 Average loss in the last 200 batches is 11104.12109375\n",
      "51800 Average loss in the last 200 batches is 11124.0888671875\n",
      "52000 Average loss in the last 200 batches is 11139.55078125\n",
      "Printing sample predictions after 52000 batches in sentence_predictions.text\n",
      "52200 Average loss in the last 200 batches is 11108.40234375\n",
      "52400 Average loss in the last 200 batches is 11139.580078125\n",
      "52600 Average loss in the last 200 batches is 11133.2490234375\n",
      "52800 Average loss in the last 200 batches is 11121.638671875\n",
      "53000 Average loss in the last 200 batches is 11156.9111328125\n",
      "53200 Average loss in the last 200 batches is 11108.0673828125\n",
      "53400 Average loss in the last 200 batches is 11087.8271484375\n",
      "53600 Average loss in the last 200 batches is 11069.2509765625\n",
      "53800 Average loss in the last 200 batches is 11110.2333984375\n",
      "54000 Average loss in the last 200 batches is 11087.958984375\n",
      "Printing sample predictions after 54000 batches in sentence_predictions.text\n",
      "54200 Average loss in the last 200 batches is 11060.87109375\n",
      "54400 Average loss in the last 200 batches is 11079.7724609375\n",
      "54600 Average loss in the last 200 batches is 11138.380859375\n",
      "54800 Average loss in the last 200 batches is 11029.1416015625\n",
      "55000 Average loss in the last 200 batches is 11076.3310546875\n",
      "55200 Average loss in the last 200 batches is 11104.6513671875\n",
      "55400 Average loss in the last 200 batches is 11086.2197265625\n",
      "55600 Average loss in the last 200 batches is 11091.169921875\n",
      "55800 Average loss in the last 200 batches is 11070.44921875\n",
      "56000 Average loss in the last 200 batches is 11086.1103515625\n",
      "Printing sample predictions after 56000 batches in sentence_predictions.text\n",
      "56200 Average loss in the last 200 batches is 11040.8515625\n",
      "56400 Average loss in the last 200 batches is 11067.611328125\n",
      "56600 Average loss in the last 200 batches is 11121.6435546875\n",
      "56800 Average loss in the last 200 batches is 11100.7099609375\n",
      "57000 Average loss in the last 200 batches is 11065.5546875\n",
      "57200 Average loss in the last 200 batches is 11064.037109375\n",
      "57400 Average loss in the last 200 batches is 11091.3447265625\n",
      "57600 Average loss in the last 200 batches is 11028.1298828125\n",
      "57800 Average loss in the last 200 batches is 11031.46875\n",
      "58000 Average loss in the last 200 batches is 11016.7275390625\n",
      "Printing sample predictions after 58000 batches in sentence_predictions.text\n",
      "58200 Average loss in the last 200 batches is 11055.26953125\n",
      "58400 Average loss in the last 200 batches is 11067.1328125\n",
      "58600 Average loss in the last 200 batches is 11003.6923828125\n",
      "58800 Average loss in the last 200 batches is 11031.5458984375\n",
      "59000 Average loss in the last 200 batches is 11018.6513671875\n",
      "59200 Average loss in the last 200 batches is 11040.6083984375\n",
      "59400 Average loss in the last 200 batches is 11045.736328125\n",
      "59600 Average loss in the last 200 batches is 11030.9033203125\n",
      "59800 Average loss in the last 200 batches is 11068.708984375\n",
      "60000 Average loss in the last 200 batches is 10987.9345703125\n",
      "Printing sample predictions after 60000 batches in sentence_predictions.text\n",
      "60200 Average loss in the last 200 batches is 11010.5625\n",
      "60400 Average loss in the last 200 batches is 10973.8701171875\n",
      "60600 Average loss in the last 200 batches is 11016.83203125\n",
      "60800 Average loss in the last 200 batches is 10979.7900390625\n",
      "61000 Average loss in the last 200 batches is 11014.9609375\n",
      "61200 Average loss in the last 200 batches is 11025.8037109375\n",
      "61400 Average loss in the last 200 batches is 10995.8466796875\n",
      "61600 Average loss in the last 200 batches is 11000.818359375\n",
      "61800 Average loss in the last 200 batches is 11014.369140625\n",
      "62000 Average loss in the last 200 batches is 11015.79296875\n",
      "Printing sample predictions after 62000 batches in sentence_predictions.text\n",
      "62200 Average loss in the last 200 batches is 10986.1083984375\n",
      "62400 Average loss in the last 200 batches is 11022.48828125\n",
      "62600 Average loss in the last 200 batches is 10939.083984375\n",
      "62800 Average loss in the last 200 batches is 10984.8603515625\n",
      "63000 Average loss in the last 200 batches is 10940.1240234375\n",
      "63200 Average loss in the last 200 batches is 11021.400390625\n",
      "63400 Average loss in the last 200 batches is 11011.5625\n",
      "63600 Average loss in the last 200 batches is 10897.8759765625\n",
      "63800 Average loss in the last 200 batches is 11022.7724609375\n",
      "64000 Average loss in the last 200 batches is 11014.056640625\n",
      "Printing sample predictions after 64000 batches in sentence_predictions.text\n",
      "64200 Average loss in the last 200 batches is 10980.4921875\n",
      "64400 Average loss in the last 200 batches is 10971.2509765625\n",
      "64600 Average loss in the last 200 batches is 10932.10546875\n",
      "64800 Average loss in the last 200 batches is 11000.396484375\n",
      "65000 Average loss in the last 200 batches is 10955.4677734375\n",
      "65200 Average loss in the last 200 batches is 10937.771484375\n",
      "65400 Average loss in the last 200 batches is 10946.0703125\n",
      "65600 Average loss in the last 200 batches is 10949.259765625\n",
      "65800 Average loss in the last 200 batches is 10997.916015625\n",
      "66000 Average loss in the last 200 batches is 10962.708984375\n",
      "Printing sample predictions after 66000 batches in sentence_predictions.text\n",
      "66200 Average loss in the last 200 batches is 10978.9853515625\n",
      "66400 Average loss in the last 200 batches is 10980.462890625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-afde5d90a0ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpadded_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpadded_in_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_de_dict_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mpadded_in_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded_in_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadded_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "num_batches = 0\n",
    "num_loss_prints = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"Starting epoch {epoch} =====================\")\n",
    "    \n",
    "    best_loss = 1e10\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for idx, sentences in enumerate(sentences_loader):\n",
    "\n",
    "        rnn_encoder.init_hidden_and_cell()\n",
    "       \n",
    "   \n",
    "        in_sentences = sentences['de_sentences']\n",
    "        in_lens = sentences['de_lens']\n",
    "        out_sentences = sentences['en_sentences']\n",
    "        out_lens = sentences['en_lens']\n",
    "        \n",
    "\n",
    "        padded_in = pad_sequence(in_sentences, padding_value=0).to(device)\n",
    "        padded_out = pad_sequence(out_sentences, padding_value=0).to(device)\n",
    "\n",
    "        padded_in_one_hot = torch.zeros(padded_in.shape[0], padded_in.shape[1], dataset.get_de_dict_size()).to(device)\n",
    "        padded_in_one_hot = padded_in_one_hot.scatter_(2,padded_in.data,1)\n",
    "       \n",
    "        rnn_encoder.forward((padded_in_one_hot, in_lens))\n",
    "        hidden, cell = rnn_encoder.get_hidden_and_cell()\n",
    "       \n",
    "        rnn_decoder.init_hidden_and_cell(hidden,cell)\n",
    "       \n",
    "        max_sentence_len = padded_out.shape[0]\n",
    "            \n",
    "        y_pred = rnn_decoder.forward(dataset.get_en_eos_code(), dataset.get_en_dict_size(), max_sentence_len)\n",
    "       \n",
    "        padded_out = padded_out[0:max_sentence_len]\n",
    "        padded_out_one_hot = torch.zeros(padded_out.shape[0], padded_out.shape[1], dataset.get_en_dict_size()).to(device)\n",
    "        padded_out_one_hot = padded_out_one_hot.scatter_(2,padded_out.data,1)\n",
    "       \n",
    "        #Make all padded one-hot vectors to all zeros, which will make\n",
    "        #padded components loss 0 and so they wont affect the loss\n",
    "        padded_out_one_hot[:,:,0] = torch.zeros(max_sentence_len, padded_out_one_hot.shape[1])\n",
    "        loss = torch.sum(-torch.log(y_pred + 1e-9) * padded_out_one_hot)\n",
    "       \n",
    "        loss_sum += loss.to('cpu').detach().data\n",
    "       \n",
    "        #print(loss.to('cpu').detach().data)\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Every 100 batches print the average loss and store the model weights\n",
    "        steps += BATCH_SIZE\n",
    "        num_batches += 1\n",
    "        loss_print_step = 200\n",
    "        if num_batches % loss_print_step == 0:\n",
    "            \n",
    "            print(f\"{num_batches} Average loss in the last {loss_print_step} batches is {loss_sum/float(loss_print_step)}\")\n",
    "            steps = 0\n",
    "            \n",
    "            \n",
    "            num_loss_prints += 1 \n",
    "            \n",
    "            if num_loss_prints % 10 == 0:\n",
    "                print_results(in_sentences, out_sentences, y_pred.to('cpu').detach().data, num_batches)\n",
    "            \n",
    "            if best_loss > loss_sum:\n",
    "                best_loss = loss_sum\n",
    "\n",
    "                models_path = \"models\"\n",
    "                if not os.path.exists(models_path):\n",
    "                    os.mkdir(models_path)\n",
    "\n",
    "                torch.save(rnn_encoder.state_dict(), trained_encoder_path)\n",
    "                torch.save(rnn_decoder.state_dict(), trained_decoder_path)\n",
    "            \n",
    "            loss_sum = 0\n",
    "            steps = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 =====================\n",
      "200 Average loss in the last 200 batches is 10807.32421875\n",
      "400 Average loss in the last 200 batches is 10749.759765625\n",
      "600 Average loss in the last 200 batches is 10706.5400390625\n",
      "800 Average loss in the last 200 batches is 10728.8974609375\n",
      "1000 Average loss in the last 200 batches is 10725.8134765625\n",
      "1200 Average loss in the last 200 batches is 10670.068359375\n",
      "1400 Average loss in the last 200 batches is 10720.861328125\n",
      "1600 Average loss in the last 200 batches is 10716.0791015625\n",
      "1800 Average loss in the last 200 batches is 10679.7021484375\n",
      "2000 Average loss in the last 200 batches is 10715.9677734375\n",
      "Printing sample predictions after 2000 batches in sentence_predictions.text\n",
      "2200 Average loss in the last 200 batches is 10706.7177734375\n",
      "2400 Average loss in the last 200 batches is 10709.73828125\n",
      "2600 Average loss in the last 200 batches is 10705.5029296875\n",
      "2800 Average loss in the last 200 batches is 10698.0166015625\n",
      "3000 Average loss in the last 200 batches is 10647.263671875\n",
      "3200 Average loss in the last 200 batches is 10673.4189453125\n",
      "3400 Average loss in the last 200 batches is 10678.8837890625\n",
      "3600 Average loss in the last 200 batches is 10662.58984375\n",
      "3800 Average loss in the last 200 batches is 10717.7578125\n",
      "4000 Average loss in the last 200 batches is 10665.9453125\n",
      "Printing sample predictions after 4000 batches in sentence_predictions.text\n",
      "4200 Average loss in the last 200 batches is 10654.6796875\n",
      "4400 Average loss in the last 200 batches is 10678.673828125\n",
      "4600 Average loss in the last 200 batches is 10645.9150390625\n",
      "4800 Average loss in the last 200 batches is 10676.537109375\n",
      "5000 Average loss in the last 200 batches is 10646.5322265625\n",
      "5200 Average loss in the last 200 batches is 10666.4296875\n",
      "5400 Average loss in the last 200 batches is 10682.3388671875\n",
      "5600 Average loss in the last 200 batches is 10657.2412109375\n",
      "5800 Average loss in the last 200 batches is 10674.701171875\n",
      "6000 Average loss in the last 200 batches is 10670.0361328125\n",
      "Printing sample predictions after 6000 batches in sentence_predictions.text\n",
      "6200 Average loss in the last 200 batches is 10630.0029296875\n",
      "6400 Average loss in the last 200 batches is 10595.6611328125\n",
      "6600 Average loss in the last 200 batches is 10642.1337890625\n",
      "6800 Average loss in the last 200 batches is 10623.37890625\n",
      "7000 Average loss in the last 200 batches is 10651.3125\n",
      "7200 Average loss in the last 200 batches is 10633.8310546875\n",
      "7400 Average loss in the last 200 batches is 10657.8798828125\n",
      "7600 Average loss in the last 200 batches is 10606.6533203125\n",
      "7800 Average loss in the last 200 batches is 10615.69140625\n",
      "8000 Average loss in the last 200 batches is 10652.763671875\n",
      "Printing sample predictions after 8000 batches in sentence_predictions.text\n",
      "8200 Average loss in the last 200 batches is 10646.4228515625\n",
      "8400 Average loss in the last 200 batches is 10628.458984375\n",
      "8600 Average loss in the last 200 batches is 10642.740234375\n",
      "8800 Average loss in the last 200 batches is 10622.330078125\n",
      "9000 Average loss in the last 200 batches is 10621.72265625\n",
      "9200 Average loss in the last 200 batches is 10632.3359375\n",
      "9400 Average loss in the last 200 batches is 10617.2626953125\n",
      "9600 Average loss in the last 200 batches is 10625.224609375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4401ec80c1d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpadded_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpadded_in_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_de_dict_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mpadded_in_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded_in_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadded_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Continue to train the same model with smaller learning rate of 1e-4 (\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr = 1e-4)\n",
    "\n",
    "steps = 0\n",
    "num_batches = 0\n",
    "num_loss_prints = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"Starting epoch {epoch} =====================\")\n",
    "    \n",
    "    best_loss = 1e10\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for idx, sentences in enumerate(sentences_loader):\n",
    "\n",
    "        rnn_encoder.init_hidden_and_cell()\n",
    "       \n",
    "   \n",
    "        in_sentences = sentences['de_sentences']\n",
    "        in_lens = sentences['de_lens']\n",
    "        out_sentences = sentences['en_sentences']\n",
    "        out_lens = sentences['en_lens']\n",
    "        \n",
    "\n",
    "        padded_in = pad_sequence(in_sentences, padding_value=0).to(device)\n",
    "        padded_out = pad_sequence(out_sentences, padding_value=0).to(device)\n",
    "\n",
    "        padded_in_one_hot = torch.zeros(padded_in.shape[0], padded_in.shape[1], dataset.get_de_dict_size()).to(device)\n",
    "        padded_in_one_hot = padded_in_one_hot.scatter_(2,padded_in.data,1)\n",
    "       \n",
    "        rnn_encoder.forward((padded_in_one_hot, in_lens))\n",
    "        hidden, cell = rnn_encoder.get_hidden_and_cell()\n",
    "       \n",
    "        rnn_decoder.init_hidden_and_cell(hidden,cell)\n",
    "       \n",
    "        max_sentence_len = padded_out.shape[0]\n",
    "            \n",
    "        y_pred = rnn_decoder.forward(dataset.get_en_eos_code(), dataset.get_en_dict_size(), max_sentence_len)\n",
    "       \n",
    "        padded_out = padded_out[0:max_sentence_len]\n",
    "        padded_out_one_hot = torch.zeros(padded_out.shape[0], padded_out.shape[1], dataset.get_en_dict_size()).to(device)\n",
    "        padded_out_one_hot = padded_out_one_hot.scatter_(2,padded_out.data,1)\n",
    "       \n",
    "        #Make all padded one-hot vectors to all zeros, which will make\n",
    "        #padded components loss 0 and so they wont affect the loss\n",
    "        padded_out_one_hot[:,:,0] = torch.zeros(max_sentence_len, padded_out_one_hot.shape[1])\n",
    "        loss = torch.sum(-torch.log(y_pred + 1e-9) * padded_out_one_hot)\n",
    "       \n",
    "        loss_sum += loss.to('cpu').detach().data\n",
    "       \n",
    "        #print(loss.to('cpu').detach().data)\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Every 200 batches print the average loss and store the model weights\n",
    "        steps += BATCH_SIZE\n",
    "        num_batches += 1\n",
    "        loss_print_step = 200\n",
    "        if num_batches % loss_print_step == 0:\n",
    "            \n",
    "            print(f\"{num_batches} Average loss in the last {loss_print_step} batches is {loss_sum/float(loss_print_step)}\")\n",
    "            steps = 0\n",
    "            \n",
    "            \n",
    "            num_loss_prints += 1 \n",
    "            \n",
    "            if num_loss_prints % 10 == 0:\n",
    "                print_results(in_sentences, out_sentences, y_pred.to('cpu').detach().data, num_batches)\n",
    "            \n",
    "            if best_loss > loss_sum:\n",
    "                best_loss = loss_sum\n",
    "\n",
    "                models_path = \"models\"\n",
    "                if not os.path.exists(models_path):\n",
    "                    os.mkdir(models_path)\n",
    "\n",
    "                torch.save(rnn_encoder.state_dict(), trained_encoder_path)\n",
    "                torch.save(rnn_decoder.state_dict(), trained_decoder_path)\n",
    "            \n",
    "            loss_sum = 0\n",
    "            steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 =====================\n",
      "9800 Average loss in the last 200 batches is 7144.40869140625\n",
      "10000 Average loss in the last 200 batches is 10564.2177734375\n",
      "10200 Average loss in the last 200 batches is 10587.638671875\n",
      "10400 Average loss in the last 200 batches is 10588.2275390625\n",
      "10600 Average loss in the last 200 batches is 10566.5234375\n",
      "10800 Average loss in the last 200 batches is 10594.4365234375\n",
      "11000 Average loss in the last 200 batches is 10523.6474609375\n",
      "11200 Average loss in the last 200 batches is 10557.849609375\n",
      "11400 Average loss in the last 200 batches is 10607.12890625\n",
      "11600 Average loss in the last 200 batches is 10611.0126953125\n",
      "Printing sample predictions after 11600 batches in sentence_predictions.text\n",
      "11800 Average loss in the last 200 batches is 10565.3251953125\n",
      "12000 Average loss in the last 200 batches is 10654.5576171875\n",
      "12200 Average loss in the last 200 batches is 10603.1640625\n",
      "12400 Average loss in the last 200 batches is 10576.9775390625\n",
      "12600 Average loss in the last 200 batches is 10605.4404296875\n",
      "12800 Average loss in the last 200 batches is 10562.099609375\n",
      "13000 Average loss in the last 200 batches is 10573.60546875\n",
      "13200 Average loss in the last 200 batches is 10591.2734375\n",
      "13400 Average loss in the last 200 batches is 10645.990234375\n",
      "13600 Average loss in the last 200 batches is 10620.419921875\n",
      "Printing sample predictions after 13600 batches in sentence_predictions.text\n",
      "13800 Average loss in the last 200 batches is 10596.6279296875\n",
      "14000 Average loss in the last 200 batches is 10558.705078125\n",
      "14200 Average loss in the last 200 batches is 10602.0390625\n",
      "14400 Average loss in the last 200 batches is 10574.681640625\n",
      "14600 Average loss in the last 200 batches is 10622.349609375\n",
      "14800 Average loss in the last 200 batches is 10549.95703125\n",
      "15000 Average loss in the last 200 batches is 10613.138671875\n",
      "15200 Average loss in the last 200 batches is 10563.3037109375\n",
      "15400 Average loss in the last 200 batches is 10548.337890625\n",
      "15600 Average loss in the last 200 batches is 10547.8115234375\n",
      "Printing sample predictions after 15600 batches in sentence_predictions.text\n",
      "15800 Average loss in the last 200 batches is 10587.83984375\n",
      "16000 Average loss in the last 200 batches is 10577.443359375\n",
      "16200 Average loss in the last 200 batches is 10559.0087890625\n",
      "16400 Average loss in the last 200 batches is 10554.8603515625\n",
      "16600 Average loss in the last 200 batches is 10632.6435546875\n",
      "16800 Average loss in the last 200 batches is 10573.7802734375\n",
      "17000 Average loss in the last 200 batches is 10603.4970703125\n",
      "17200 Average loss in the last 200 batches is 10535.955078125\n",
      "17400 Average loss in the last 200 batches is 10571.77734375\n",
      "17600 Average loss in the last 200 batches is 10642.9189453125\n",
      "Printing sample predictions after 17600 batches in sentence_predictions.text\n",
      "17800 Average loss in the last 200 batches is 10640.0927734375\n",
      "18000 Average loss in the last 200 batches is 10587.0771484375\n",
      "18200 Average loss in the last 200 batches is 10554.9560546875\n",
      "18400 Average loss in the last 200 batches is 10561.86328125\n",
      "18600 Average loss in the last 200 batches is 10590.01953125\n",
      "18800 Average loss in the last 200 batches is 10622.125\n",
      "19000 Average loss in the last 200 batches is 10567.1298828125\n",
      "19200 Average loss in the last 200 batches is 10589.8662109375\n",
      "19400 Average loss in the last 200 batches is 10603.1796875\n",
      "19600 Average loss in the last 200 batches is 10559.4853515625\n",
      "Printing sample predictions after 19600 batches in sentence_predictions.text\n",
      "19800 Average loss in the last 200 batches is 10574.791015625\n",
      "20000 Average loss in the last 200 batches is 10609.31640625\n",
      "20200 Average loss in the last 200 batches is 10613.01171875\n",
      "20400 Average loss in the last 200 batches is 10612.5390625\n",
      "20600 Average loss in the last 200 batches is 10577.3408203125\n",
      "20800 Average loss in the last 200 batches is 10643.3251953125\n",
      "21000 Average loss in the last 200 batches is 10559.740234375\n",
      "21200 Average loss in the last 200 batches is 10561.712890625\n",
      "21400 Average loss in the last 200 batches is 10626.61328125\n",
      "21600 Average loss in the last 200 batches is 10599.072265625\n",
      "Printing sample predictions after 21600 batches in sentence_predictions.text\n",
      "21800 Average loss in the last 200 batches is 10583.080078125\n",
      "22000 Average loss in the last 200 batches is 10617.224609375\n",
      "22200 Average loss in the last 200 batches is 10576.6572265625\n",
      "22400 Average loss in the last 200 batches is 10590.1015625\n",
      "22600 Average loss in the last 200 batches is 10608.494140625\n",
      "22800 Average loss in the last 200 batches is 10557.6337890625\n",
      "23000 Average loss in the last 200 batches is 10554.798828125\n",
      "23200 Average loss in the last 200 batches is 10562.1240234375\n",
      "23400 Average loss in the last 200 batches is 10606.662109375\n",
      "23600 Average loss in the last 200 batches is 10534.6923828125\n",
      "Printing sample predictions after 23600 batches in sentence_predictions.text\n",
      "23800 Average loss in the last 200 batches is 10642.1474609375\n",
      "24000 Average loss in the last 200 batches is 10603.58203125\n",
      "24200 Average loss in the last 200 batches is 10636.849609375\n",
      "24400 Average loss in the last 200 batches is 10561.94921875\n",
      "24600 Average loss in the last 200 batches is 10623.3125\n",
      "24800 Average loss in the last 200 batches is 10539.2685546875\n",
      "25000 Average loss in the last 200 batches is 10576.7333984375\n",
      "25200 Average loss in the last 200 batches is 10596.705078125\n",
      "25400 Average loss in the last 200 batches is 10543.2587890625\n",
      "25600 Average loss in the last 200 batches is 10597.875\n",
      "Printing sample predictions after 25600 batches in sentence_predictions.text\n",
      "25800 Average loss in the last 200 batches is 10580.5185546875\n",
      "26000 Average loss in the last 200 batches is 10574.2021484375\n",
      "26200 Average loss in the last 200 batches is 10608.7158203125\n",
      "26400 Average loss in the last 200 batches is 10531.60546875\n",
      "26600 Average loss in the last 200 batches is 10526.5185546875\n",
      "26800 Average loss in the last 200 batches is 10596.79296875\n",
      "27000 Average loss in the last 200 batches is 10591.9453125\n",
      "27200 Average loss in the last 200 batches is 10617.7734375\n",
      "27400 Average loss in the last 200 batches is 10482.0361328125\n",
      "27600 Average loss in the last 200 batches is 10559.318359375\n",
      "Printing sample predictions after 27600 batches in sentence_predictions.text\n",
      "27800 Average loss in the last 200 batches is 10593.68359375\n",
      "28000 Average loss in the last 200 batches is 10545.23046875\n",
      "28200 Average loss in the last 200 batches is 10543.544921875\n",
      "28400 Average loss in the last 200 batches is 10611.8486328125\n",
      "28600 Average loss in the last 200 batches is 10581.392578125\n",
      "28800 Average loss in the last 200 batches is 10584.943359375\n",
      "29000 Average loss in the last 200 batches is 10593.1728515625\n",
      "29200 Average loss in the last 200 batches is 10593.349609375\n",
      "29400 Average loss in the last 200 batches is 10544.5263671875\n",
      "29600 Average loss in the last 200 batches is 10560.1025390625\n",
      "Printing sample predictions after 29600 batches in sentence_predictions.text\n",
      "29800 Average loss in the last 200 batches is 10615.1025390625\n",
      "30000 Average loss in the last 200 batches is 10564.47265625\n",
      "30200 Average loss in the last 200 batches is 10597.7802734375\n",
      "30400 Average loss in the last 200 batches is 10585.3896484375\n",
      "30600 Average loss in the last 200 batches is 10572.337890625\n",
      "30800 Average loss in the last 200 batches is 10591.5048828125\n",
      "31000 Average loss in the last 200 batches is 10583.3486328125\n",
      "31200 Average loss in the last 200 batches is 10568.28515625\n",
      "31400 Average loss in the last 200 batches is 10563.236328125\n",
      "31600 Average loss in the last 200 batches is 10569.3173828125\n",
      "Printing sample predictions after 31600 batches in sentence_predictions.text\n",
      "31800 Average loss in the last 200 batches is 10532.8271484375\n",
      "32000 Average loss in the last 200 batches is 10572.9248046875\n",
      "32200 Average loss in the last 200 batches is 10595.7529296875\n",
      "32400 Average loss in the last 200 batches is 10559.2890625\n",
      "32600 Average loss in the last 200 batches is 10535.89453125\n",
      "32800 Average loss in the last 200 batches is 10560.736328125\n",
      "33000 Average loss in the last 200 batches is 10570.7802734375\n",
      "33200 Average loss in the last 200 batches is 10586.80078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33400 Average loss in the last 200 batches is 10601.86328125\n",
      "33600 Average loss in the last 200 batches is 10552.1201171875\n",
      "Printing sample predictions after 33600 batches in sentence_predictions.text\n",
      "33800 Average loss in the last 200 batches is 10566.4296875\n",
      "34000 Average loss in the last 200 batches is 10544.1611328125\n",
      "34200 Average loss in the last 200 batches is 10555.4111328125\n",
      "34400 Average loss in the last 200 batches is 10603.4716796875\n",
      "34600 Average loss in the last 200 batches is 10574.5322265625\n",
      "34800 Average loss in the last 200 batches is 10580.744140625\n",
      "35000 Average loss in the last 200 batches is 10578.3525390625\n",
      "35200 Average loss in the last 200 batches is 10586.884765625\n",
      "35400 Average loss in the last 200 batches is 10589.6533203125\n",
      "35600 Average loss in the last 200 batches is 10606.4423828125\n",
      "Printing sample predictions after 35600 batches in sentence_predictions.text\n",
      "35800 Average loss in the last 200 batches is 10574.8154296875\n",
      "36000 Average loss in the last 200 batches is 10583.599609375\n",
      "36200 Average loss in the last 200 batches is 10528.4033203125\n",
      "36400 Average loss in the last 200 batches is 10562.9189453125\n",
      "36600 Average loss in the last 200 batches is 10570.1689453125\n",
      "36800 Average loss in the last 200 batches is 10512.7421875\n",
      "37000 Average loss in the last 200 batches is 10558.0048828125\n",
      "37200 Average loss in the last 200 batches is 10535.46875\n",
      "37400 Average loss in the last 200 batches is 10563.34375\n",
      "37600 Average loss in the last 200 batches is 10587.5146484375\n",
      "Printing sample predictions after 37600 batches in sentence_predictions.text\n",
      "37800 Average loss in the last 200 batches is 10547.7685546875\n",
      "38000 Average loss in the last 200 batches is 10568.205078125\n",
      "38200 Average loss in the last 200 batches is 10593.6435546875\n",
      "38400 Average loss in the last 200 batches is 10579.31640625\n",
      "38600 Average loss in the last 200 batches is 10617.369140625\n",
      "38800 Average loss in the last 200 batches is 10549.4296875\n",
      "39000 Average loss in the last 200 batches is 10540.802734375\n",
      "39200 Average loss in the last 200 batches is 10562.2646484375\n",
      "39400 Average loss in the last 200 batches is 10593.494140625\n",
      "39600 Average loss in the last 200 batches is 10545.177734375\n",
      "Printing sample predictions after 39600 batches in sentence_predictions.text\n",
      "39800 Average loss in the last 200 batches is 10560.0302734375\n",
      "40000 Average loss in the last 200 batches is 10516.052734375\n",
      "40200 Average loss in the last 200 batches is 10572.0712890625\n",
      "40400 Average loss in the last 200 batches is 10533.9013671875\n",
      "40600 Average loss in the last 200 batches is 10570.7763671875\n",
      "40800 Average loss in the last 200 batches is 10577.4033203125\n",
      "41000 Average loss in the last 200 batches is 10592.9658203125\n",
      "41200 Average loss in the last 200 batches is 10603.40625\n",
      "41400 Average loss in the last 200 batches is 10563.1328125\n",
      "41600 Average loss in the last 200 batches is 10577.3583984375\n",
      "Printing sample predictions after 41600 batches in sentence_predictions.text\n",
      "41800 Average loss in the last 200 batches is 10556.9326171875\n",
      "42000 Average loss in the last 200 batches is 10571.55078125\n",
      "42200 Average loss in the last 200 batches is 10588.6513671875\n",
      "42400 Average loss in the last 200 batches is 10491.787109375\n",
      "42600 Average loss in the last 200 batches is 10571.1728515625\n",
      "42800 Average loss in the last 200 batches is 10581.53125\n",
      "43000 Average loss in the last 200 batches is 10577.1123046875\n",
      "43200 Average loss in the last 200 batches is 10525.3076171875\n",
      "43400 Average loss in the last 200 batches is 10595.294921875\n",
      "43600 Average loss in the last 200 batches is 10570.94140625\n",
      "Printing sample predictions after 43600 batches in sentence_predictions.text\n",
      "43800 Average loss in the last 200 batches is 10558.9287109375\n",
      "44000 Average loss in the last 200 batches is 10627.052734375\n",
      "44200 Average loss in the last 200 batches is 10620.50390625\n",
      "44400 Average loss in the last 200 batches is 10555.7626953125\n",
      "Starting epoch 1 =====================\n",
      "44600 Average loss in the last 200 batches is 1216.1959228515625\n",
      "44800 Average loss in the last 200 batches is 10562.724609375\n",
      "45000 Average loss in the last 200 batches is 10529.4365234375\n",
      "45200 Average loss in the last 200 batches is 10556.28125\n",
      "45400 Average loss in the last 200 batches is 10526.5634765625\n",
      "45600 Average loss in the last 200 batches is 10517.5126953125\n",
      "Printing sample predictions after 45600 batches in sentence_predictions.text\n",
      "45800 Average loss in the last 200 batches is 10561.7275390625\n",
      "46000 Average loss in the last 200 batches is 10617.3427734375\n",
      "46200 Average loss in the last 200 batches is 10624.3583984375\n",
      "46400 Average loss in the last 200 batches is 10544.4853515625\n",
      "46600 Average loss in the last 200 batches is 10540.3349609375\n",
      "46800 Average loss in the last 200 batches is 10584.607421875\n",
      "47000 Average loss in the last 200 batches is 10525.201171875\n",
      "47200 Average loss in the last 200 batches is 10573.03125\n",
      "47400 Average loss in the last 200 batches is 10566.9775390625\n",
      "47600 Average loss in the last 200 batches is 10540.4423828125\n",
      "Printing sample predictions after 47600 batches in sentence_predictions.text\n",
      "47800 Average loss in the last 200 batches is 10540.1279296875\n",
      "48000 Average loss in the last 200 batches is 10556.287109375\n",
      "48200 Average loss in the last 200 batches is 10551.5888671875\n",
      "48400 Average loss in the last 200 batches is 10590.107421875\n",
      "48600 Average loss in the last 200 batches is 10547.5615234375\n",
      "48800 Average loss in the last 200 batches is 10552.892578125\n",
      "49000 Average loss in the last 200 batches is 10580.072265625\n",
      "49200 Average loss in the last 200 batches is 10542.1845703125\n",
      "49400 Average loss in the last 200 batches is 10584.6650390625\n",
      "49600 Average loss in the last 200 batches is 10555.3212890625\n",
      "Printing sample predictions after 49600 batches in sentence_predictions.text\n",
      "49800 Average loss in the last 200 batches is 10518.7724609375\n",
      "50000 Average loss in the last 200 batches is 10538.4453125\n",
      "50200 Average loss in the last 200 batches is 10581.177734375\n",
      "50400 Average loss in the last 200 batches is 10547.0146484375\n",
      "50600 Average loss in the last 200 batches is 10543.0810546875\n",
      "50800 Average loss in the last 200 batches is 10587.1025390625\n",
      "51000 Average loss in the last 200 batches is 10521.880859375\n",
      "51200 Average loss in the last 200 batches is 10538.337890625\n",
      "51400 Average loss in the last 200 batches is 10514.603515625\n",
      "51600 Average loss in the last 200 batches is 10582.4609375\n",
      "Printing sample predictions after 51600 batches in sentence_predictions.text\n",
      "51800 Average loss in the last 200 batches is 10493.1826171875\n",
      "52000 Average loss in the last 200 batches is 10560.958984375\n",
      "52200 Average loss in the last 200 batches is 10542.8564453125\n",
      "52400 Average loss in the last 200 batches is 10543.3896484375\n",
      "52600 Average loss in the last 200 batches is 10588.85546875\n",
      "52800 Average loss in the last 200 batches is 10545.88671875\n",
      "53000 Average loss in the last 200 batches is 10535.955078125\n",
      "53200 Average loss in the last 200 batches is 10482.12109375\n",
      "53400 Average loss in the last 200 batches is 10602.5302734375\n",
      "53600 Average loss in the last 200 batches is 10511.2548828125\n",
      "Printing sample predictions after 53600 batches in sentence_predictions.text\n",
      "53800 Average loss in the last 200 batches is 10528.205078125\n",
      "54000 Average loss in the last 200 batches is 10509.2421875\n",
      "54200 Average loss in the last 200 batches is 10569.9873046875\n",
      "54400 Average loss in the last 200 batches is 10557.6474609375\n",
      "54600 Average loss in the last 200 batches is 10591.4951171875\n",
      "54800 Average loss in the last 200 batches is 10578.4599609375\n",
      "55000 Average loss in the last 200 batches is 10576.0029296875\n",
      "55200 Average loss in the last 200 batches is 10550.361328125\n",
      "55400 Average loss in the last 200 batches is 10529.98046875\n",
      "55600 Average loss in the last 200 batches is 10526.33984375\n",
      "Printing sample predictions after 55600 batches in sentence_predictions.text\n",
      "55800 Average loss in the last 200 batches is 10609.6591796875\n",
      "56000 Average loss in the last 200 batches is 10516.2060546875\n",
      "56200 Average loss in the last 200 batches is 10520.2802734375\n",
      "56400 Average loss in the last 200 batches is 10575.138671875\n",
      "56600 Average loss in the last 200 batches is 10544.0224609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56800 Average loss in the last 200 batches is 10553.919921875\n",
      "57000 Average loss in the last 200 batches is 10580.888671875\n",
      "57200 Average loss in the last 200 batches is 10554.2724609375\n",
      "57400 Average loss in the last 200 batches is 10559.7822265625\n",
      "57600 Average loss in the last 200 batches is 10523.25\n",
      "Printing sample predictions after 57600 batches in sentence_predictions.text\n",
      "57800 Average loss in the last 200 batches is 10527.16796875\n",
      "58000 Average loss in the last 200 batches is 10530.142578125\n",
      "58200 Average loss in the last 200 batches is 10532.5859375\n",
      "58400 Average loss in the last 200 batches is 10584.2666015625\n",
      "58600 Average loss in the last 200 batches is 10549.6279296875\n",
      "58800 Average loss in the last 200 batches is 10562.8486328125\n",
      "59000 Average loss in the last 200 batches is 10489.9912109375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d215f93c9b47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmax_sentence_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_en_eos_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_en_dict_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sentence_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mpadded_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_sentence_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b3dd43d4ce28>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, out_eos_code, out_dict_size, max_sentence_len)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sentence_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mprev_outp_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_outp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_outp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dict_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mprev_outp_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_outp_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_outp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Continue to train the same model with even smaller learning rate of 1e-5 (\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr = 1e-5)\n",
    "\n",
    "steps = 0\n",
    "num_loss_prints = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"Starting epoch {epoch} =====================\")\n",
    "    \n",
    "    best_loss = 1e10\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for idx, sentences in enumerate(sentences_loader):\n",
    "\n",
    "        rnn_encoder.init_hidden_and_cell()\n",
    "       \n",
    "   \n",
    "        in_sentences = sentences['de_sentences']\n",
    "        in_lens = sentences['de_lens']\n",
    "        out_sentences = sentences['en_sentences']\n",
    "        out_lens = sentences['en_lens']\n",
    "        \n",
    "\n",
    "        padded_in = pad_sequence(in_sentences, padding_value=0).to(device)\n",
    "        padded_out = pad_sequence(out_sentences, padding_value=0).to(device)\n",
    "\n",
    "        padded_in_one_hot = torch.zeros(padded_in.shape[0], padded_in.shape[1], dataset.get_de_dict_size()).to(device)\n",
    "        padded_in_one_hot = padded_in_one_hot.scatter_(2,padded_in.data,1)\n",
    "       \n",
    "        rnn_encoder.forward((padded_in_one_hot, in_lens))\n",
    "        hidden, cell = rnn_encoder.get_hidden_and_cell()\n",
    "       \n",
    "        rnn_decoder.init_hidden_and_cell(hidden,cell)\n",
    "       \n",
    "        max_sentence_len = padded_out.shape[0]\n",
    "            \n",
    "        y_pred = rnn_decoder.forward(dataset.get_en_eos_code(), dataset.get_en_dict_size(), max_sentence_len)\n",
    "       \n",
    "        padded_out = padded_out[0:max_sentence_len]\n",
    "        padded_out_one_hot = torch.zeros(padded_out.shape[0], padded_out.shape[1], dataset.get_en_dict_size()).to(device)\n",
    "        padded_out_one_hot = padded_out_one_hot.scatter_(2,padded_out.data,1)\n",
    "       \n",
    "        #Make all padded one-hot vectors to all zeros, which will make\n",
    "        #padded components loss 0 and so they wont affect the loss\n",
    "        padded_out_one_hot[:,:,0] = torch.zeros(max_sentence_len, padded_out_one_hot.shape[1])\n",
    "        loss = torch.sum(-torch.log(y_pred + 1e-9) * padded_out_one_hot)\n",
    "       \n",
    "        loss_sum += loss.to('cpu').detach().data\n",
    "       \n",
    "        #print(loss.to('cpu').detach().data)\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Every 200 batches print the average loss and store the model weights\n",
    "        steps += BATCH_SIZE\n",
    "        num_batches += 1\n",
    "        loss_print_step = 200\n",
    "        if num_batches % loss_print_step == 0:\n",
    "            \n",
    "            print(f\"{num_batches} Average loss in the last {loss_print_step} batches is {loss_sum/float(loss_print_step)}\")\n",
    "            steps = 0\n",
    "            \n",
    "            \n",
    "            num_loss_prints += 1 \n",
    "            \n",
    "            if num_loss_prints % 10 == 0:\n",
    "                print_results(in_sentences, out_sentences, y_pred.to('cpu').detach().data, num_batches)\n",
    "            \n",
    "            if best_loss > loss_sum:\n",
    "                best_loss = loss_sum\n",
    "\n",
    "                models_path = \"models\"\n",
    "                if not os.path.exists(models_path):\n",
    "                    os.mkdir(models_path)\n",
    "\n",
    "                torch.save(rnn_encoder.state_dict(), trained_encoder_path)\n",
    "                torch.save(rnn_decoder.state_dict(), trained_decoder_path)\n",
    "            \n",
    "            loss_sum = 0\n",
    "            steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 =====================\n",
      "59200 Average loss in the last 200 batches is 1776.3555908203125\n",
      "59400 Average loss in the last 200 batches is 10538.2021484375\n",
      "59600 Average loss in the last 200 batches is 10581.0322265625\n",
      "59800 Average loss in the last 200 batches is 10599.4912109375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr = 1e-4)\n",
    "\n",
    "steps = 0\n",
    "num_loss_prints = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"Starting epoch {epoch} =====================\")\n",
    "    \n",
    "    best_loss = 1e10\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for idx, sentences in enumerate(sentences_loader):\n",
    "\n",
    "        rnn_encoder.init_hidden_and_cell()\n",
    "       \n",
    "   \n",
    "        in_sentences = sentences['de_sentences']\n",
    "        in_lens = sentences['de_lens']\n",
    "        out_sentences = sentences['en_sentences']\n",
    "        out_lens = sentences['en_lens']\n",
    "        \n",
    "\n",
    "        padded_in = pad_sequence(in_sentences, padding_value=0).to(device)\n",
    "        padded_out = pad_sequence(out_sentences, padding_value=0).to(device)\n",
    "\n",
    "        padded_in_one_hot = torch.zeros(padded_in.shape[0], padded_in.shape[1], dataset.get_de_dict_size()).to(device)\n",
    "        padded_in_one_hot = padded_in_one_hot.scatter_(2,padded_in.data,1)\n",
    "       \n",
    "        rnn_encoder.forward((padded_in_one_hot, in_lens))\n",
    "        hidden, cell = rnn_encoder.get_hidden_and_cell()\n",
    "       \n",
    "        rnn_decoder.init_hidden_and_cell(hidden,cell)\n",
    "       \n",
    "        max_sentence_len = padded_out.shape[0]\n",
    "            \n",
    "        y_pred = rnn_decoder.forward(dataset.get_en_eos_code(), dataset.get_en_dict_size(), max_sentence_len)\n",
    "       \n",
    "        padded_out = padded_out[0:max_sentence_len]\n",
    "        padded_out_one_hot = torch.zeros(padded_out.shape[0], padded_out.shape[1], dataset.get_en_dict_size()).to(device)\n",
    "        padded_out_one_hot = padded_out_one_hot.scatter_(2,padded_out.data,1)\n",
    "       \n",
    "        #Make all padded one-hot vectors to all zeros, which will make\n",
    "        #padded components loss 0 and so they wont affect the loss\n",
    "        padded_out_one_hot[:,:,0] = torch.zeros(max_sentence_len, padded_out_one_hot.shape[1])\n",
    "        loss = torch.sum(-torch.log(y_pred + 1e-9) * padded_out_one_hot)\n",
    "       \n",
    "        loss_sum += loss.to('cpu').detach().data\n",
    "       \n",
    "        #print(loss.to('cpu').detach().data)\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Every 200 batches print the average loss and store the model weights\n",
    "        steps += BATCH_SIZE\n",
    "        num_batches += 1\n",
    "        loss_print_step = 200\n",
    "        if num_batches % loss_print_step == 0:\n",
    "            \n",
    "            print(f\"{num_batches} Average loss in the last {loss_print_step} batches is {loss_sum/float(loss_print_step)}\")\n",
    "            steps = 0\n",
    "            \n",
    "            \n",
    "            num_loss_prints += 1 \n",
    "            \n",
    "            if num_loss_prints % 10 == 0:\n",
    "                print_results(in_sentences, out_sentences, y_pred.to('cpu').detach().data, num_batches)\n",
    "            \n",
    "            if best_loss > loss_sum:\n",
    "                best_loss = loss_sum\n",
    "\n",
    "                models_path = \"models\"\n",
    "                if not os.path.exists(models_path):\n",
    "                    os.mkdir(models_path)\n",
    "\n",
    "                torch.save(rnn_encoder.state_dict(), trained_encoder_path)\n",
    "                torch.save(rnn_decoder.state_dict(), trained_decoder_path)\n",
    "            \n",
    "            loss_sum = 0\n",
    "            steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
